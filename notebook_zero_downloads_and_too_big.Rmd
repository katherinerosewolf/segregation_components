---
title: "Segregation Components Paper Code"
author: "Katherine Rose Wolf"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

NOTES FOR KATIE: 5110ish, transpose that section (individual tracts)


# CODE SETUP (run every time)

```{r rmarkdown setup}

knitr::opts_chunk$set(echo = TRUE)

```


```{r load packages}

library(rmarkdown)
library(stringi)
library(knitr)
library(plyr)
library(data.table)
library(Hmisc)
library(extrafontdb)
library(extrafont)
library(NADA)
library(grid)
library(gridExtra)
library(lattice)
library(tmap)
library(sf)
library(tidycensus)
library(tigris)
library(rgeos)
library(sjPlot)
library(stargazer)
library(readxl)
library(tidyverse)
library(totalcensus)
library(spdep)
library(lubridate)
library(lwgeom)
library(ggdag)
library(furrr)
library(pryr)

```


```{r set up future parallel processing options}

# future::plan(strategy = "multiprocess")

```


```{r establish directory structure}

if(!dir.exists("raw_data")) {
  dir.create("raw_data")
}

if(!dir.exists(file.path("raw_data", "totalcensus"))) {
  dir.create(file.path("raw_data", "totalcensus"))
}

if(!dir.exists("pre_travis_intermediate_data")) {
  dir.create("pre_travis_intermediate_data")
}

if(!dir.exists("travis_cache")) {
  dir.create("travis_cache")
}

if(!dir.exists(file.path("functions"))) {
  dir.create(file.path("functions"))
}

```


```{r census api key and options for tidycensus and totalcensus}

# install the census api key for tidycensus
census_api_key(key = Sys.getenv("CENSUS_API_KEY"), 
               install = FALSE, 
               overwrite = FALSE)

readRenviron("~/.Renviron")

# have tidycensus store old shapefiles for future use
options(tigris_use_cache = TRUE)

# set path to census data for totalcensus
Sys.setenv(PATH_TO_CENSUS = "raw_data/totalcensus")

```


```{r functions for general later use}

# intermediate object saver
intermediate_object_saver <- 
  function(...) {
    object_names <- tibble::lst(...)
    saver <- 
      function(object_name){
        save(list = as.character(object_name),
             file = file.path("intermediate_data",
                              paste0(object_name,
                                     ".rdata")), 
             envir = .GlobalEnv)
      }
    map(.x = object_names,
        .f = saver)
  }

# intermediate object loader
intermediate_object_loader <- 
  function(...) {
    object_names <- tibble::lst(...)
    loader <- 
      function(object_name){
        load(file = file.path("intermediate_data",
                              paste0(object_name,
                                     ".rdata")),
             envir = .GlobalEnv)
      }
    map(.x = object_names,
        .f = loader)
  }

# function to convert column names to snake case
column_name_fixer <-
  function(tibble) {
  new_names <-
    tibble %>%
    colnames() %>%  # gets the column names from the tibble
    tolower() %>%  # converts the column names to lowercase
    {gsub(" ", "_", .)}  # replaces spaces with "_"
  colnames(tibble) <-
    new_names  # rename the columns in the original tibble
  return(tibble)
  }

# create function that downloads files
file_downloader <-
  function(desired_filename_as_string, 
           web_address_as_string, 
           folder_as_string) {
    
    path <-  # specify the file path
      file.path(
        folder_as_string,
        desired_filename_as_string) 
    
    if(!file.exists(path))  # checks if file exists already
      download.file(url = web_address_as_string,  # if not, downloads/saves file
                    destfile = path, 
                    mode = "wb")
  }

# function to download generated data without asking
download_generated_data_auto <- 
  function() {
    cat(paste("Downloading data generated from decennial census 2010."))
    total_files <- 426
    path_to_census <- Sys.getenv("PATH_TO_CENSUS")
    url <- "https://s3.amazonaws.com/gl-shared-data/generated_census_data_v060.zip"
    download.file(url, paste0(path_to_census, "/tmp.zip"))
    unzip(paste0(path_to_census, "/tmp.zip"), exdir = paste0(path_to_census, 
        "/generated_data"))
    file.remove(paste0(path_to_census, "/tmp.zip"))
    n_files <- length(list.files(paste0(path_to_census, "/generated_data"), 
        recursive = TRUE))
    if (n_files == total_files) {
        cat("Extraction is successful!\n\n")
    }
    else {
        cat("Last downloading or extraction has a problem. Download and extract again.")
        download_generated_data_auto()
    }
  }

# save functions for later use
save(column_name_fixer, 
     file = file.path("functions", 
                      "column_name_fixer.rdata"))

save(file_downloader, 
     file = file.path("functions", 
                      "file_downloader.rdata"))

save(download_generated_data_auto, 
     file = file.path("functions", 
                      "download_generated_data_auto.rdata"))

save(intermediate_object_saver,
     file = file.path("functions", 
                      "intermediate_object_saver.rdata"))

save(intermediate_object_loader,
     file = file.path("functions", 
                      "intermediate_object_loader.rdata"))

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


# TOO BIG FOR TRAVIS

## geography

### states for analyses

```{r make state abbreviation character vector}

# check if file already exists
if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "state_abbreviation_character_vector.rdata"))) {
  
# make vector of state abbreviations (includes PR, no VI)
state_abbreviation_character_vector <- 
  c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN", "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")

# save to disk
save(state_abbreviation_character_vector, 
     file = file.path("pre_travis_intermediate_data", 
                      "state_abbreviation_character_vector.rdata"))

# remove it
rm(state_abbreviation_character_vector)
invisible(gc())

}

```


### census full shapefile without data

```{r census detailed shapefile without data raw}

# source https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_%s_tract10.zip
# where %s is the state FIPS code
# downloaded 27 June 2019
# last modified 23 June 2011 (state 60, others earlier)

if(
  !file.exists(
    file.path(
      "raw_data", 
      "tract_shapefiles_raw.rdata"
    )
    )
  ) {
  
  # load vector of state abbreviations
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
  )
  
  # download the data
  tract_shapefiles_raw <- 
    rbind_tigris(
    map(
      state_abbreviation_character_vector, 
      function(x) {
        tracts(
          x, 
          cb = FALSE, 
          year = 2010, 
          class = 'sf'
          )
        }
      )
  )
  
  save(
    tract_shapefiles_raw, 
    file = 
      file.path(
        "raw_data", 
        "tract_shapefiles_raw.rdata"
      )
  )
  
  # remove loaded and created objects from environment and clear memory
  rm(
    state_abbreviation_character_vector,
    tract_shapefiles_raw
  )
  invisible(gc())
}

```


```{r fix tract shapefiles to be pretty}

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "tract_shapefiles.rdata"
    )
    )
  ) {

  # load the raw data
  load(
    file = 
      file.path(
       "raw_data", 
       "tract_shapefiles_raw.rdata"
      )
    )
  
  # load the column name fixer function
  load(
    file = 
      file.path(
       "functions", 
       "column_name_fixer.rdata"
      )
    )
  
  # make the fixes
  tract_shapefiles <- 
    tract_shapefiles_raw %>% 
    as_tibble() %>%  # convert to a tibble
    st_as_sf() %>%  # convert back to an sf
    column_name_fixer # %>%   # fix column names
    # st_set_precision(precision = 100)  # set the precision to 100
  
  # save the result to disk
  save(
    tract_shapefiles, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "tract_shapefiles.rdata"
      )
  )
  
  # remove loaded and created objects from environment and clear memory
  rm(
    column_name_fixer,
    tract_shapefiles_raw,
    tract_shapefiles
  )
  invisible(gc())
}

```


### census cartographic boundary file without data

```{r census cartographic boundary file without data raw}

# source https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_%s_tract10.zip
# where %s is the state FIPS code
# downloaded 27 June 2019
# last modified 23 June 2011 (state 60, others earlier)

if(
  !file.exists(
    file.path(
      "raw_data", 
      "tract_cbfs_raw.rdata"
    )
    )
  ) {
  
  # load vector of state abbreviations
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
  )
  
  # download the data
  tract_cbfs_raw <- 
    rbind_tigris(
    map(
      state_abbreviation_character_vector, 
      function(x) {
        tracts(
          x, 
          cb = TRUE, 
          year = 2010, 
          class = 'sf'
          )
        }
      )
  )
  
  save(
    tract_cbfs_raw, 
    file = 
      file.path(
        "raw_data", 
        "tract_cbfs_raw.rdata"
      )
  )
  
  # remove loaded and created objects from environment and clear memory
  rm(
    state_abbreviation_character_vector,
    tract_cbfs_raw
  )
  invisible(gc())
}
```


```{r fix census cartographic boundary files to be pretty}

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "tract_cbfs.rdata"
    )
    )
  ) {

  # load the raw data
  load(
    file = 
      file.path(
       "raw_data", 
       "tract_cbfs_raw.rdata"
      )
    )
  
  # load the column name fixer function
  load(
    file = 
      file.path(
       "functions", 
       "column_name_fixer.rdata"
      )
    )
  
  # make the fixes
  tract_cbfs <- 
    tract_cbfs_raw %>% 
    as_tibble() %>%  # convert to a tibble
    st_as_sf() %>%  # convert back to an sf
    column_name_fixer %>%   # fix column names
    # st_set_precision(precision = 100) %>%  # set the precision to 100 (messes up geometries)
    mutate(  # create geoid10 to match full shapefiles
      geoid10 = 
        str_replace(
        geo_id,
        "1400000US", 
        ""
      )
    )
  
  # save the result to disk
  save(
    tract_cbfs, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "tract_cbfs.rdata"
      )
  )
  
  # remove loaded and created objects from environment
  rm(
    column_name_fixer,
    tract_cbfs_raw,
    tract_cbfs
  )
  
  # clear memory
  invisible(gc())
  
}
  
```


### neighbor lists

```{r make list of tract neighbors for later segregation}

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "neighbor_tibble_long.rdata"
    )
  )
) {

  # load the census tract shapefiles
  load(
    file =
      file.path(
        "pre_travis_intermediate_data",
        "tract_shapefiles.rdata"
      )
    )
  
  # get character vector for numbers from 1 to the number of tracts
  tract_order_character <- 
    as.character(1:nrow(tract_shapefiles))
  
  # pull the geoids as a vector
  tigris_geoid_character <- 
    tract_shapefiles %>% 
    pull(geoid10)
  
  # make tibble indexing tracts in order to match geoids with the spdep output
  geoid_index_tibble <- 
    tibble(index = tract_order_character, 
           geoid = tigris_geoid_character)
    
  if(!file.exists(file.path("pre_travis_intermediate_data", 
                            "tract_neighbor_nb.rdata"))){
    
    # make a list of lists of neighbors to each tract (uses spdep package)
    tract_neighbor_nb <-  
      poly2nb(pl = tract_shapefiles, 
              queen = TRUE)
  
    # name the elements of the big list above with their tract indices
    names(tract_neighbor_nb) <- tract_order_character
    
    # save it to disk
    save(
      tract_neighbor_nb,
      file = 
        file.path(
          "pre_travis_intermediate_data", 
          "tract_neighbor_nb.rdata"
        )
    )
  
  } else {
    
    # load the file
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "tract_neighbor_nb.rdata"
      )
    )
  }
  
  # check if the neighbors-as-one-string tibble already exists
  if(!file.exists(file.path(
    "pre_travis_intermediate_data", 
    "neighbors_in_one_string_tibble.rdata"))){
    
  # if not, create it
  neighbors_in_one_string_tibble <- 
    map_df(  # makes a tibble from the output of poly2nb()
      tract_neighbor_nb, 
      paste,  # puts all the adjacent tracts into one string
      collapse = ","  # separated by commas
      ) %>%  # makes two columns from one row with column names
    gather(key = "primary_tract",  
           value = "adjacent_tracts") %>% 
    mutate(  # adds column of the primary tract and its neighbors in one string
      primary_and_adjacent_tracts = 
        paste(primary_tract, 
              adjacent_tracts, 
              sep = ",")
    )
  
    # save the result
    save(neighbors_in_one_string_tibble,
           file = file.path("pre_travis_intermediate_data", 
                            "neighbors_in_one_string_tibble.rdata"))
    
  } else {  
    
    # otherwise load the relevant tibble
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "neighbors_in_one_string_tibble.rdata"
      )
    )
  }
  
  # make list of character vectors of neighbor tract indices
  # ordered by primary tract index
  split_tracts <- 
    neighbors_in_one_string_tibble %>% 
    pull(primary_and_adjacent_tracts) %>%  # pull the self and neighbor column
    str_split(pattern = ",")  # split strings of neighbor indices
  
  # name each list element by its primary tract
  names(split_tracts) <- 
    neighbors_in_one_string_tibble$primary_tract
  
  # make long tibble repeating the primary tract for itself and every neighbor
  neighbor_tibble_long <- 
    as_tibble(  # makes it a tibble repeating the primary (index) for each neighbor
      stack(split_tracts)[2:1]) %>%  # reorders them so that the primary tract comes first
    rename(primary_tract = 1,  # renames the columns with meaningful names
           neighbor_tract = 2) %>% 
    mutate(  # coerce primary tract column (index) back to character
      primary_tract = as.character(primary_tract)
      ) %>% 
    left_join(
      geoid_index_tibble,  # join with the original geoids corresponding
      by = c("primary_tract" = "index")  # to the primary tract
      ) %>%  
    rename(geoid_primary_tract = geoid) %>%  # rename geoid for primary tract
    left_join(
      geoid_index_tibble,  # join with the original geoids corresponding
      by = c("neighbor_tract" = "index")  # to the neighbor tract
      ) %>%  
    rename(geoid_neighbor_tract = geoid) # rename geoid for neighbor tract
  
  save(
    neighbor_tibble_long, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "neighbor_tibble_long.rdata"
      )
    )
  
  # remove all the files
  rm(
    tract_shapefiles, 
    tract_neighbor_nb,
    tract_order_character, 
    tigris_geoid_character,
    geoid_index_tibble,
    neighbors_in_one_string_tibble,
    split_tracts,
    neighbor_tibble_long
  )
  invisible(gc())
}

```


```{r make CBF-based list of tract neighbors}

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "cbf_neighbor_tibble_long.rdata"
    )
  )
) {

  # load the census tract shapefiles
  load(
    file =
      file.path(
        "pre_travis_intermediate_data",
        "tract_cbfs.rdata"
      )
    )
  
  # get character vector for numbers from 1 to the number of tracts
  cbf_tract_order_character <- 
    as.character(1:nrow(tract_cbfs))
  
  # pull the geoids as a vector
  cbf_tigris_geoid_character <- 
    tract_cbfs %>% 
    pull(geoid10)
  
  # make tibble indexing tracts in order to match geoids with the spdep output
  cbf_geoid_index_tibble <- 
    tibble(index = cbf_tract_order_character, 
           geoid = cbf_tigris_geoid_character)
    
  if(!file.exists(file.path("pre_travis_intermediate_data", 
                            "cbf_tract_neighbor_nb.rdata"))){
    
    # make a list of lists of neighbors to each tract (uses spdep package)
    cbf_tract_neighbor_nb <-  
      poly2nb(pl = tract_cbfs, 
              queen = TRUE)
  
    # name the elements of the big list above with their tract indices
    names(cbf_tract_neighbor_nb) <- cbf_tract_order_character
    
    # save the file to disk
    save(
      cbf_tract_neighbor_nb, 
      file = 
        file.path(
          "pre_travis_intermediate_data", 
          "cbf_tract_neighbor_nb.rdata"
        )
    )
  
  } else {
    
    # load the file
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "cbf_tract_neighbor_nb.rdata"
      )
    )
  }
  
  # check if the neighbors-as-one-string tibble already exists
  if(!file.exists(file.path(
    "pre_travis_intermediate_data", 
    "cbf_neighbors_in_one_string_tibble.rdata"))){
    
  # if not, create it
  cbf_neighbors_in_one_string_tibble <- 
    map_df(  # makes a tibble from the output of poly2nb()
      cbf_tract_neighbor_nb, 
      paste,  # puts all the adjacent tracts into one string
      collapse = ","  # separated by commas
      ) %>%  # makes two columns from one row with column names
    gather(key = "primary_tract",  
           value = "adjacent_tracts") %>% 
    mutate(  # adds column of the primary tract and its neighbors in one string
      primary_and_adjacent_tracts = 
        paste(primary_tract, 
              adjacent_tracts, 
              sep = ",")
    )
  
    # save the result
    save(cbf_neighbors_in_one_string_tibble,
           file = file.path("pre_travis_intermediate_data", 
                            "cbf_neighbors_in_one_string_tibble.rdata"))
    
  } else {  
    
    # otherwise load the relevant tibble
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "cbf_neighbors_in_one_string_tibble.rdata"
      )
    )
  }
  
  # make list of character vectors of neighbor tract indices
  # ordered by primary tract index
  cbf_split_tracts <- 
    cbf_neighbors_in_one_string_tibble %>% 
    pull(primary_and_adjacent_tracts) %>%  # pull the self and neighbor column
    str_split(pattern = ",")  # split strings of neighbor indices
  
  # name each list element by its primary tract
  names(cbf_split_tracts) <- 
    cbf_neighbors_in_one_string_tibble$primary_tract
  
  # make long tibble repeating the primary tract for itself and every neighbor
  cbf_neighbor_tibble_long <- 
    as_tibble(  # makes it a tibble repeating the primary (index) for each neighbor
      stack(cbf_split_tracts)[2:1]) %>%  # reorders them so that the primary tract comes first
    rename(primary_tract = 1,  # renames the columns with meaningful names
           neighbor_tract = 2) %>% 
    mutate(  # coerce primary tract column (index) back to character
      primary_tract = as.character(primary_tract)
      ) %>% 
    left_join(
      cbf_geoid_index_tibble,  # join with the original geoids corresponding
      by = c("primary_tract" = "index")  # to the primary tract
      ) %>%  
    rename(geoid_primary_tract = geoid) %>%  # rename geoid for primary tract
    left_join(
      cbf_geoid_index_tibble,  # join with the original geoids corresponding
      by = c("neighbor_tract" = "index")  # to the neighbor tract
      ) %>%  
    rename(geoid_neighbor_tract = geoid) # rename geoid for neighbor tract
  
  save(
    cbf_neighbor_tibble_long, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "cbf_neighbor_tibble_long.rdata"
      )
    )
  
  # remove all the files
  rm(
    tract_cbfs, 
    cbf_tract_neighbor_nb,
    cbf_tract_order_character, 
    cbf_tigris_geoid_character,
    cbf_geoid_index_tibble,
    cbf_neighbors_in_one_string_tibble,
    cbf_split_tracts,
    cbf_neighbor_tibble_long
  )
  invisible(gc())
}

```



## demographic and ses data

### census

#### census variable lists

```{r census variable lists}

# check if variable list already exists
if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "sf1_totalcensus_var_vec.rdata"))) {

  # # view census variables
  # census_variables_sf1 <- 
  #   load_variables(
  #     2010, 
  #     "sf1", 
  #     cache = TRUE
  #     )
  # census_variables_sf3 <- 
  #   load_variables(
  #     2000, 
  #     "sf3", 
  #     cache = TRUE
  #     )
  # acs_variables <- 
  #   load_variables(
  #     2012, 
  #     "acs", 
  #     cache = TRUE
  #   )
  
  # define categories for variables
  
  # total population (SF1 P1)
  # universe: total population
  total_pop_sf1_p1 <-
    c("P0010001")
  
  # hispanic or latino origin by race (SF1 P5)
  # universe: total population
  race_sf1_p5 <-
    sprintf("P0050%0.3d", 1:17)
  
  # P0050001  Total population:
  # P0050002    Not Hispanic or Latino:
  # P0050003      White alone
  # P0050004      Black or African American alone
  # P0050005      American Indian and Alaska Native alone
  # P0050006      Asian alone
  # P0050007      Native Hawaiian and Other Pacific Islander alone
  # P0050008      Some Other Race alone
  # P0050009      Two or More Races
  # P0050010    Hispanic or Latino:
  # P0050011      White alone
  # P0050012      Black or African American alone
  # P0050013      American Indian and Alaska Native alone
  # P0050014      Asian alone
  # P0050015      Native Hawaiian and Other Pacific Islander alone
  # P0050016      Some Other Race alone
  # P0050017      Two or More Races
  
  # race (SF1 PCT23)
  # Universe: Total population
  race_sf1_pct23 <- 
    sprintf("PCT023%0.4d", 1:24)
  
  # PCT0230001  Total: 
  # PCT0230002  Population of one race: 
  # PCT0230003    White 
  # PCT0230004    Black or African American 
  # PCT0230005    American Indian and Alaska Native 
  # PCT0230006    Asian: 
  # PCT0230007      Asian Indian 
  # PCT0230008      Chinese (including Taiwanese) 
  # PCT0230009      Filipino 
  # PCT0230010      Japanese 
  # PCT0230011      Korean 
  # PCT0230012      Vietnamese 
  # PCT0230013      Other Asian 
  # PCT0230014    Native Hawaiian and Other Pacific Islander: 
  # PCT0230015      Native Hawaiian 
  # PCT0230016      Guamanian or Chamorro 
  # PCT0230017      Samoan 
  # PCT0230018      Other Pacific Islander 
  # PCT0230019    Some Other Race 
  # PCT0230020    Population of Two or More Races 
  # PCT0230021      White; American Indian and Alaska Native 
  # PCT0230022      White; Asian 
  # PCT0230023      White; Black or African American 
  # PCT0230024      White; Some Other Race 
  
  # hispanic or latino by specific orgiin (SF1, PCT11) [31]
  # Universe: Total population
  hisp_lat_origin_sf1_pct11 <- 
    sprintf("PCT011%0.4d", 1:31)
  
  # PCT0110001  Total:
  # PCT0110002    Not Hispanic or Latino (001–199, 300–999) 
  # PCT0110003    Hispanic or Latino (200–299): 
  # PCT0110004      Mexican (210–220) 
  # PCT0110005      Puerto Rican (260–269) 
  # PCT0110006      Cuban (270–274) 
  # PCT0110007      Dominican (275–279) 
  # PCT0110008      Central American (excluding Mexican) (221–230): 
  # PCT0110009        Costa Rican (221) 
  # PCT0110010        Guatemalan (222) 
  # PCT0110011        Honduran (223) 
  # PCT0110012        Nicaraguan (224) 
  # PCT0110013        Panamanian (225) 
  # PCT0110014        Salvadoran (226) 
  # PCT0110015        Other Central American (227–230) 
  # PCT0110016      South American (231–249): 
  # PCT0110017        Argentinean (231) 
  # PCT0110018        Bolivian (232) 
  # PCT0110019        Chilean (233) 
  # PCT0110020        Colombian (234) 
  # PCT0110021        Ecuadorian (235) 
  # PCT0110022        Paraguayan (236) 
  # PCT0110023        Peruvian (237) 
  # PCT0110024        Uruguayan (238) 
  # PCT0110025        Venezuelan (239) 
  # PCT0110026        Other South American (240–249) 
  # PCT0110027      Other Hispanic or Latino (200–209, 250–259, 280–299): 
  # PCT0110028        Spaniard (200–209) 
  # PCT0110029        Spanish (282) 
  # PCT0110030        Spanish American (286) 
  # PCT0110031        All other Hispanic or Latino (250–259, 280–281, 283–285, 287–299) 
  
  
  # sex by age (SF1 P12)
  # universe: total population
  sex_by_age_sf1_p12 <- 
    sprintf("P0120%0.3d", 1:49)
  # NOTE: P0120002 is all male and P0120026 is all female
  
  # P0120001  Total population:
  # P0120002    Male:
  # P0120003      Under 5 years
  # P0120004      5 to 9 years
  # P0120005      10 to 14 years
  # P0120006      15 to 17 years
  # P0120007      18 and 19 years
  # P0120008      20 years
  # P0120009      21 years
  # P0120010      22 to 24 years
  # P0120011      25 to 29 years
  # P0120012      30 to 34 years
  # P0120013      35 to 39 years
  # P0120014      40 to 44 years
  # P0120015      45 to 49 years
  # P0120016      50 to 54 years
  # P0120017      55 to 59 years
  # P0120018      60 and 61 years
  # P0120019      62 to 64 years
  # P0120020      65 and 66 years
  # P0120021      67 to 69 years
  # P0120022      70 to 74 years
  # P0120023      75 to 79 years
  # P0120024      80 to 84 years
  # P0120025      85 years and over
  # P0120026    Female:
  # P0120027      Under 5 years
  # P0120028      5 to 9 years
  # P0120029      10 to 14 years
  # P0120030      15 to 17 years
  # P0120031      18 and 19 years
  # P0120032      20 years
  # P0120033      21 years
  # P0120034      22 to 24 years
  # P0120035      25 to 29 years
  # P0120036      30 to 34 years
  # P0120037      35 to 39 years
  # P0120038      40 to 44 years
  # P0120039      45 to 49 years
  # P0120040      50 to 54 years
  # P0120041      55 to 59 years
  # P0120042      60 and 61 years
  # P0120043      62 to 64 years
  # P0120044      65 and 66 years
  # P0120045      67 to 69 years
  # P0120046      70 to 74 years
  # P0120047      75 to 79 years
  # P0120048      80 to 84 years
  # P0120049      85 years and over
  
  # median age (SF1 P13)
  # universe: total population
  age_median_sf1_p13 <- 
    "P0130001"
  
  # occupancy status
  # universe: housing units
  # 1 total, 2 occupied, 3 vacant
  occupancy_sf1_h3 <-
    sprintf("H00300%0.2d", 1:3)
  
  # housing tenure (SF1 H4)
  # universe: occupied housing units
  # 1 total, 2 owned w/ a mortgage or loan, 3 owned free and clear, 4 renter-occupied
  housing_tenure_sf1_h4 <-  
    sprintf("H00400%0.2d", 1:4)
  
  # population in occupied housing units by tenure
  # universe: population in occupied housing units
  housing_tenure_people_sf1_h11 <- 
    c("H0110001",  # total population in occupied housing units
      "H0110002",  # owned with a mortgage or a loan 
      "H0110003",  # owned free and clear
      "H0110004")  # renter-occupied
  
  # make list of needed variables
  sf1_totalcensus_var_list <- 
    list(
      total_pop_sf1_p1,
      race_sf1_p5,
      race_sf1_pct23, 
      hisp_lat_origin_sf1_pct11,
      sex_by_age_sf1_p12,
      age_median_sf1_p13,
      occupancy_sf1_h3,
      housing_tenure_sf1_h4,
      housing_tenure_people_sf1_h11
    )
  
  # make big master variable list
  sf1_totalcensus_var_vec <- 
    sf1_totalcensus_var_list %>% 
    reduce(c)
  
  # save the master variable list
  save(
    sf1_totalcensus_var_vec, 
    file = 
      file.path("pre_travis_intermediate_data", 
                "sf1_totalcensus_var_vec.rdata")
  )
  
  rm(
    total_pop_sf1_p1,
    race_sf1_p5,
    sex_by_age_sf1_p12,
    age_median_sf1_p13,
    occupancy_sf1_h3,
    housing_tenure_sf1_h4,
    housing_tenure_people_sf1_h11, 
    sf1_totalcensus_var_list, 
    sf1_totalcensus_var_vec
    )
  invisible(gc())
}

```


#### census totalcensus variable pulls

```{r sf1 totalcensus data download}

# header information from
# file source: https://www2.census.gov/census_2010/04-Summary_File_1/Urban_Rural_Update/
# individual state files from there
# downloaded 2019-06-27
# last updated per website 2012-09-27

# check if the raw sf1 totalcensus tibble exists and if so, skip the whole chunk
if(!
   file.exists(
     file.path(
       "raw_data", 
       "sf1_totalcensus_raw_dataframe.rdata"
     )
   )
   ) {
  
  # load state abbreviation character vector
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
  )
  
  # load census variable names
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "sf1_totalcensus_var_vec.rdata")
  )
  
  # download the data necessary to use the census data if it isn't already there
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "generated_data"
        )
      )
    ) 
  download_generated_data_auto()
  
  # check if 2010 census data is there
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "census2010"
        )
      )
    ) {
    
    # download the census data if it's not
      download_census(
        survey = "dec",
        year = 2010, 
        states = state_abbreviation_character_vector
        )
  }
  
  # create dataframe of needed variables
  sf1_totalcensus_raw_dataframe <-
    read_decennial(
      year = 2010,
      states = state_abbreviation_character_vector,
      table_contents =
        sf1_totalcensus_var_vec,
      summary_level = "tract"
  )
  
  # save the tibble to disk
  save(
    sf1_totalcensus_raw_dataframe, 
    file = 
      file.path(
        "raw_data",
        "sf1_totalcensus_raw_dataframe.rdata"
        )
    )
  
  # remove working files
  rm(
    sf1_totalcensus_var_vec,
    state_abbreviation_character_vector, 
    sf1_totalcensus_raw_dataframe
    )
  invisible(gc())
}

```


### acs

#### acs variable lists

```{r acs totalcensus variable lists}

if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "acs_totalcensus_var_vec.rdata"))) {

  # educational attainment for the population 25 years and over
  # universe: population 25 years and over
  edu_cat_b15003 <- 
    c("B15003_001",  # total
      "B15003_002",  # no schooling completed
      "B15003_003",  # nursery school
      "B15003_004",  # kindergarten
      "B15003_005",  # 1st grade
      "B15003_006",  # 2nd grade
      "B15003_007",  # 3rd grade
      "B15003_008",  # 4th grade
      "B15003_009",  # 5th grade
      "B15003_010",  # 6th grade
      "B15003_011",  # 7th grade
      "B15003_012",  # 8th grade
      "B15003_013",  # 9th grade
      "B15003_014",  # 10th grade
      "B15003_015",  # 11th grade
      "B15003_016",  # 12th grade
      "B15003_017",  # regular high school diploma
      "B15003_018",  # GED or alternative credential
      "B15003_019",  # some college, less than 1 year
      "B15003_020",  # some college, 1 or more years, no degree
      "B15003_021",  # associate's degree
      "B15003_022",  # bachelor's degree
      "B15003_023",  # master's degree
      "B15003_024",  # professional school degree
      "B15003_025")  # doctorate degree
  
  # ratio of income to poverty level in the past 12 months
  # universe: population for whom poverty status is determined
  poverty_ratio_cat_c17002 <-
    c("C17002_001",  # total
      "C17002_002",  # under .50
      "C17002_003",  # .50 to .99
      "C17002_004",  # 1.00 to 1.24
      "C17002_005",  # 1.25 to 1.49
      "C17002_006",  # 1.50 to 1.84
      "C17002_007",  # 1.85 to 1.99
      "C17002_008")  # 2.00 and over
  
  # household income in the past 12 months (in 2012 inflation-adjusted dollars)
  # universe: households
  income_house_cat_b19001 <-
    c("B19001_001",  # total
      "B19001_002",  # less than $10,000
      "B19001_003",  # $10,000 to $14,999
      "B19001_004",  # $15,000 to $19,999
      "B19001_005",  # $20,000 to $24,999
      "B19001_006",  # $25,000 to $29,999
      "B19001_007",  # $30,000 to $34,999
      "B19001_008",  # $35,000 to $39,999
      "B19001_009",  # $40,000 to $44,999
      "B19001_010",  # $45,000 to $49,999
      "B19001_011",  # $50,000 to $59,999
      "B19001_012",  # $60,000 to $74,999
      "B19001_013",  # $75,000 to $99,999
      "B19001_014",  # $100,000 to $124,999
      "B19001_015",  # $125,000 to $149,999
      "B19001_016",  # $150,000 to $199,999
      "B19001_017")  # $200,000 or more
  
  # median household income in the past 12 months (in 2012 inflation-adjusted dollars)
  # universe: households
  income_house_median_b19013 <-
    c("B19013_001")
  
  # sex by earnings in the past 12 months (in 2012 inflation-adjusted dollars) for the population 16 years and over with earnings in the past 12 months
  # universe: population 16 years and over with earnings
  earnings_sex_cat_b20001 <-
    c("B20001_001",  # total
      "B20001_002",  # male
      "B20001_003",  # $1 to $2,499 or loss
      "B20001_004",  # $2,500 to $4,999
      "B20001_005",  # $5,000 to $7,499
      "B20001_006",  # $7,500 to $9,999
      "B20001_007",  # $10,000 to $12,499
      "B20001_008",  # $12,500 to $14,999
      "B20001_009",  # $15,000 to $17,499
      "B20001_010",  # $17,500 to $19,999
      "B20001_011",  # $20,000 to $22,499
      "B20001_012",  # $22,500 to $24,999
      "B20001_013",  # $25,000 to $29,999
      "B20001_014",  # $30,000 to $34,999
      "B20001_015",  # $35,000 to $39,999
      "B20001_016",  # $40,000 to $44,999
      "B20001_017",  # $45,000 to $49,999
      "B20001_018",  # $50,000 to $54,999
      "B20001_019",  # $55,000 to $64,999
      "B20001_020",  # $65,000 to $74,999
      "B20001_021",  # $75,000 to $99,999
      "B20001_022",  # $100,000 or more
      "B20001_023",  # Female:
      "B20001_024",  # $1 to $2,499 or loss
      "B20001_025",  # $2,500 to $4,999
      "B20001_026",  # $5,000 to $7,499
      "B20001_027",  # $7,500 to $9,999
      "B20001_028",  # $10,000 to $12,499
      "B20001_029",  # $12,500 to $14,999
      "B20001_030",  # $15,000 to $17,499
      "B20001_031",  # $17,500 to $19,999
      "B20001_032",  # $20,000 to $22,499
      "B20001_033",  # $22,500 to $24,999
      "B20001_034",  # $25,000 to $29,999
      "B20001_035",  # $30,000 to $34,999
      "B20001_036",  # $35,000 to $39,999
      "B20001_037",  # $40,000 to $44,999
      "B20001_038",  # $45,000 to $49,999
      "B20001_039",  # $50,000 to $54,999
      "B20001_040",  # $55,000 to $64,999
      "B20001_041",  # $65,000 to $74,999
      "B20001_042",  # $75,000 to $99,999
      "B20001_043")  # $100,000 or more
  
  # median earnings in the past 12 months (in 2012 inflation-adjusted dollars) by sex for the population 16 years and over with earnings in the past 12 months
  # universe: population 16 years and over with earnings
  earnings_median_b20002 <-
    c("B20002_001",  # total
      "B20002_002",  # male
      "B20002_003")  # female
  
  # employment status for the population 16 years and over
  # universe: population 16 years and over
  employment_cat_b23025 <-  # divide 5 by 3
    c("B23025_001",  # total
      "B23025_002",    # in labor force
      "B23025_003",      # civilian labor force
      "B23025_004",        # employed
      "B23025_005",        # unemployed
      "B23025_006",      # armed forces
      "B23025_007")    # not in labor force
    
  # value
  # universe: owner-occupied housing units
  home_value_cat_b25075 <-
    c("B25075_001",  # total
      "B25075_002",  # less than $10,000
      "B25075_003",  # $10,000 to $14,999
      "B25075_004",  # $15,000 to $19,999
      "B25075_005",  # $20,000 to $24,999
      "B25075_006",  # $25,000 to $29,999
      "B25075_007",  # $30,000 to $34,999
      "B25075_008",  # $35,000 to $39,999
      "B25075_009",  # $40,000 to $49,999
      "B25075_010",  # $50,000 to $59,999
      "B25075_011",  # $60,000 to $69,999
      "B25075_012",  # $70,000 to $79,999
      "B25075_013",  # $80,000 to $89,999
      "B25075_014",  # $90,000 to $99,999
      "B25075_015",  # $100,000 to $124,999
      "B25075_016",  # $125,000 to $149,999
      "B25075_017",  # $150,000 to $174,999
      "B25075_018",  # $175,000 to $199,999
      "B25075_019",  # $200,000 to $249,999
      "B25075_020",  # $250,000 to $299,999
      "B25075_021",  # $300,000 to $399,999
      "B25075_022",  # $400,000 to $499,999
      "B25075_023",  # $500,000 to $749,999
      "B25075_024",  # $750,000 to $999,999
      "B25075_025")  # $1,000,000 or more
  
  # median value (dollars)
  # universe: owner-occupied housing units
  home_value_median_b25077 <-
    c("B25077_001")  # median value (dollars)
  
  # types of health insurance coverage by age
  # universe: civilian noninstitutionalized population
  health_insurance_cat_c27010 <-
    c("C27010_001",  # total
      "C27010_002",    # under 18 years
      "C27010_003",      # with private health insurance only
      "C27010_004",      # with public coverage only
      "C27010_005",      # with both private and public coverage
      "C27010_006",      # no health insurance coverage
      "C27010_007",    # 18 to 34 years
      "C27010_008",      # with private health insurance only
      "C27010_009",      # with public coverage only
      "C27010_010",      # with both private and public coverage
      "C27010_011",      # no health insurance coverage
      "C27010_012",    # 35 to 64 years
      "C27010_013",      # with private health insurance only
      "C27010_014",      # with public coverage only
      "C27010_015",      # with both private and public coverage
      "C27010_016",      # no health insurance coverage
      "C27010_017",    # 65 years and over
      "C27010_018",      # with private health insurance only
      "C27010_019",      # with public coverage only
      "C27010_020",      # with both private and public coverage
      "C27010_021")      # no health insurance coverage
  
  # household language by households in which no one 14 and over speaks english only or speaks a language other than english at home and speaks english "very well"
  # universe: households
  ling_iso_b16002 <-
    c("B16002_001",  # total
      "B16002_002",    # English only
      "B16002_003",    # Spanish:
      "B16002_004",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_005",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_006",    # Other Indo-European languages:
      "B16002_007",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_008",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_009",    # Asian and Pacific Island languages:
      "B16002_010",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_011",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_012",    # Other languages: 
      "B16002_013",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_014")     # At least one person 14 and over speaks English only or speaks English "very well"
  
  # make list of needed variables
  acs_totalcensus_var_list <- 
    list(
      edu_cat_b15003,
      poverty_ratio_cat_c17002,
      income_house_cat_b19001,
      income_house_median_b19013,
      earnings_sex_cat_b20001,
      earnings_median_b20002,
      employment_cat_b23025,
      home_value_cat_b25075,
      home_value_median_b25077,
      health_insurance_cat_c27010,
      ling_iso_b16002
    )
  
  # make big master variable list
  acs_totalcensus_var_vec <-
    acs_totalcensus_var_list %>% 
    reduce(c)
    
    # save the master variable list
  save(
    acs_totalcensus_var_vec, 
    file = 
      file.path("pre_travis_intermediate_data", 
                "acs_totalcensus_var_vec.rdata")
  )
  
  rm(
    edu_cat_b15003,
    poverty_ratio_cat_c17002,
    income_house_cat_b19001,
    income_house_median_b19013,
    earnings_sex_cat_b20001,
    earnings_median_b20002,
    employment_cat_b23025,
    home_value_cat_b25075,
    home_value_median_b25077,
    health_insurance_cat_c27010,
    ling_iso_b16002,
    acs_totalcensus_var_list, 
    acs_totalcensus_var_vec
    )
  invisible(gc())
}

```

#### acs totalcensus pulls

```{r acs totalcensus data download}

# data downloaded from https://www2.census.gov/programs-surveys/acs/summary_file/2012/data/5_year_by_state/ (then individual state files)
# downloaded 2019-06-27
# last update per website 2015-02-27

# check if the acs totalcensus tibble exists and skip the whole chunk if not
if(!
   file.exists(
     file.path(
       "raw_data", 
       "acs_totalcensus_raw_dataframe.rdata"
     )
   )
   ) {

  # load variable names for reading
  load(
    file = 
      file.path("pre_travis_intermediate_data", 
                "acs_totalcensus_var_vec.rdata")
  )
  
  # load state abbreviation character vector
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
    )
  
  # download generated data that totalcensus requires if haven't already
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "generated_data"
        )
      )
    )
    download_generated_data_auto()
  
  # download actual acs data if haven't already
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "acs5year", 
        "2012"
        )
      )
    ) {
    download_census(
      survey = "acs5", 
      year = 2012, 
      states = "US"
      )
  }
  
  # this downloads the actual data!
  acs_totalcensus_raw_dataframe <-
    read_acs5year(
      year = 2012,
      states = state_abbreviation_character_vector,
      table_contents =
        acs_totalcensus_var_vec,
      summary_level = "tract",
      with_margin = TRUE
  )
  
  # save raw dataframe to disk
  save(
    acs_totalcensus_raw_dataframe, 
    file = 
      file.path(
        "raw_data", 
        "acs_totalcensus_raw_dataframe.rdata"
      )
  )
  
  # remove working files
  rm(
    acs_totalcensus_var_vec,
    state_abbreviation_character_vector, 
    acs_totalcensus_raw_dataframe
  )
  invisible(gc())
}


```


### historical income

```{r historical income table download}

# source https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls
# downloaded 2019-07-09 (ET)
# last updated 2018-08-28 per https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-income-households.html

# check if the historical income file exists and skip the whole chunk if not
if(!
   file.exists(
     file.path(
       "raw_data", 
       "inc_hh_hist_raw.rdata"
     )
   )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # download the historical income file from the internet
  file_downloader(
    desired_filename_as_string = "inc_hh_hist_raw.xls", 
    web_address_as_string = 
      "https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls", 
    folder_as_string = "raw_data"
    )
  
  # read the data from the excel file (current dollars only)
  inc_hh_hist_raw <- 
    read_xls(
      path = 
        file.path(
          "raw_data", 
          "inc_hh_hist_raw.xls"
          ), 
      range = "A7:G58", 
      col_names = 
        c("year", 
          "number_thousands", 
          "lowest", 
          "second", 
          "third", 
          "fourth", 
          "lower_limit_top_5_pct")
      )
  
  # save the data
  save(
    inc_hh_hist_raw, 
    file = 
      file.path(
        "raw_data", 
        "inc_hh_hist_raw.rdata"
      )
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    inc_hh_hist_raw
    )
  invisible(gc())
}
   
   
   
```


### ruca

```{r ruca data download}

# source 
# https://www.ers.usda.gov/webdocs/DataFiles/53241/ruca2010revised.xlsx?v=8632.5
# downloaded 2019-07-12
# last updated 2019-07-03 per 
# https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes.aspx

if(
  !file.exists(
    file.path(
      "raw_data", 
      "ruca_2010_raw.rdata"
      )
    )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # download files
  file_downloader(
    desired_filename_as_string = "ruca_2010_raw.xlsx", 
    web_address_as_string = 
      "https://www.ers.usda.gov/webdocs/DataFiles/53241/ruca2010revised.xlsx?v=8632.5", 
    folder_as_string = "raw_data"
    )
  
  # read the data from the excel file
  ruca_2010_raw <- 
    read_xlsx(
      file.path(
        "raw_data", 
        "ruca_2010_raw.xlsx"
        )
      )
  
  # save the excel file
  save(
    ruca_2010_raw, 
    file = 
      file.path(
        "raw_data", 
        "ruca_2010_raw.rdata"
      )
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    ruca_2010_raw
    )
  invisible(gc())
}

```


## air pollution

### pm2.5

```{r pm2.5 total frm}

# source https://aqs.epa.gov/aqsweb/airdata/ (see vector below for exact)
# downloaded 2019 June 19
# last updated 2019 May 28 (all files)

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "raw_pm_25_frm_tibble.rdata"
      )
    )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # load the column name fixer function
  load(
    file = 
      file.path(
       "functions", 
       "column_name_fixer.rdata"
      )
    )

  # make list of files to download
  pm_25_frm_files <- 
    c("https://aqs.epa.gov/aqsweb/airdata/daily_88101_2005.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2006.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2007.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2008.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2009.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2010.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2011.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2012.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2013.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2014.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2015.zip")
  
  
  # make a list of filenames for the downloaded data using the above filenames
  pm_25_frm_zip_filename_list <-  # make list of what the filenames will be
    map_chr(pm_25_frm_files,  # witness the `map` function
            str_extract, 
            "daily_.+\\.zip")   # witness the regular expression!
  
  # download the files
  map2(pm_25_frm_zip_filename_list, 
       pm_25_frm_files, 
       file_downloader, 
       folder_as_string = "raw_data")
  
  # unzip all the files and save them to disk
  map(
    file.path(
      "raw_data",
      pm_25_frm_zip_filename_list
      ),
    unzip,
    exdir = "pre_travis_intermediate_data"
    )
  
  # replace ".zip" with ".csv" in pm_25_frm_zip_filename_list
  pm_25_frm_csv_filename_list <-
    map(pm_25_frm_zip_filename_list,
        str_replace,
        "zip",
        "csv")
  
  # make list of tibble names by removing the ".zip" extension
  pm_25_frm_tibble_name_list <-
    map(pm_25_frm_zip_filename_list,
        str_replace,
        ".zip",
        "")
  
  # read .csv files into tibbles!
  if(
    !file.exists(
      file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
      )
    ) {  # checks if tibbles already exist
    pm_25_frm_tibble_list <-  # if not reads .csv files into tibbles
      map(
        file.path(
          "pre_travis_intermediate_data",
          pm_25_frm_csv_filename_list),
        read_csv
        )
    names(pm_25_frm_tibble_list) <-  # names the tibbles
      
      pm_25_frm_tibble_name_list
    save(
      pm_25_frm_tibble_list,  # saves the tibbles to disk to avoid doing this again
      file = file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
      )
    
  } else {
      load(
        file = file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
        )  # loads the tibbles if they don't already exist
  }
  
  # fix column names
  pm_25_frm_tibble_list <-
    map(pm_25_frm_tibble_list,
        column_name_fixer)
  
  # combine air data tibbles into one big tibble
  raw_pm_25_frm_tibble <-
    bind_rows(pm_25_frm_tibble_list)
  
  # save all pm2.5 total data to disk
  save(
      raw_pm_25_frm_tibble,  # saves the tibbles to disk to avoid doing this again
      file = file.path(
        "pre_travis_intermediate_data",
        "raw_pm_25_frm_tibble.rdata"
        )
      )
  
  # memory and disk clearance
  
  # delete individual .csv files from disk
  map(
    file.path(
      "pre_travis_intermediate_data",
      pm_25_frm_csv_filename_list
      ),
    file.remove
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    column_name_fixer,
    pm_25_frm_csv_filename_list, 
    pm_25_frm_tibble_name_list, 
    pm_25_frm_files,
    pm_25_frm_zip_filename_list,
    pm_25_frm_tibble_list, 
    raw_pm_25_frm_tibble
    )
  
  # clear memory
  invisible(gc())

}

```


### species

```{r speciation file download via speciator function}

# source https://aqs.epa.gov/aqsweb/airdata/ (see vector below for exact)
# downloaded 2019 June 19
# last updated 2019 May 28 (all files)

# make speciator function to download speciation files
speciator <- 
  function(speciation_file_link) {
    
    # extract the file name from the link
    speciation_zip_filename <-
      str_extract(
        speciation_file_link,
        "daily_.+\\.zip"
        )
    
    # create rdata file name
    speciation_rdata_filename <- 
      str_replace(
        speciation_zip_filename,
        "zip",
        "rdata"
        )
    
     if(
      !file.exists(  # checks if tibble already exists
        file.path(
          "pre_travis_intermediate_data",
          speciation_rdata_filename
          )
        )
      ) {
       
      # load file downloader function
      load(
        file = 
          file.path(
           "functions", 
           "file_downloader.rdata"
          )
        )
       
       # load the column name fixer function
       load(
         file = 
           file.path(
             "functions", 
             "column_name_fixer.rdata"
             )
         )
       
       # download the file
       file_downloader(
         desired_filename_as_string = speciation_zip_filename,
         web_address_as_string = speciation_file_link,
         folder_as_string = "raw_data"
       )
    
       # unzip the file
       unzip(
         file.path(
           "raw_data",
           speciation_zip_filename
           ), 
         exdir = "pre_travis_intermediate_data"
         )
      
       # replace ".zip" with ".csv" in speciation_zip_filename
       speciation_csv_filename <- 
         str_replace(
           speciation_zip_filename, 
           "zip", 
           "csv")
      
       speciation_tibble_name <- 
         str_replace(
           speciation_zip_filename, 
           ".zip",
           ""
         )
      
       # read .csv file into a tibble
         speciation_tibble <- 
           read_csv(
             file.path(
               "pre_travis_intermediate_data",
               speciation_csv_filename
               )
             )
         
       # fix column names
       speciation_tibble <-
         column_name_fixer(
           speciation_tibble
         )
       
       # rename it to its filename
       assign(
         speciation_tibble_name, 
         speciation_tibble
       )
        
       # save the result to disk
       save(
         list = speciation_tibble_name,
         file = 
           file.path(
             "pre_travis_intermediate_data",
             speciation_rdata_filename
             )
       )
       
       # delete .csv file from disk
       map(
         file.path(
           "pre_travis_intermediate_data",
           speciation_csv_filename
           ),
         file.remove
         )
      
       # remove loaded and created objects from environment
       rm(
         file_downloader,
         column_name_fixer,
         speciation_tibble
         )
       invisible(gc())
     }
    
    # remove files
    rm(
      speciation_zip_filename, 
      speciation_rdata_filename
    )
    invisible(gc())
  }

# make list of speciation web addresses
speciation_files <-
  c("https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2005.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2006.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2007.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2008.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2009.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2010.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2011.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2012.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2013.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2014.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2015.zip")

map(
  .x = speciation_files, 
  .f = speciator
)

rm(speciation_files)
invisible(gc())

```


```{r species big tibble}

if(
  !file.exists(  # checks if tibble already exists
    file.path(
      "pre_travis_intermediate_data",
      "raw_speciation_tibble.rdata"
      )
    )
  ) {
  # make list of speciation file names except the file names of the first two
  speciation_filenames_except_2005_2006 <- 
    sprintf("daily_SPEC_%d.rdata", 
            seq(2007, 2015, 1))

  # load 2005 speciation file
  load(
    file.path(
      "pre_travis_intermediate_data", 
      "daily_SPEC_2005.rdata"
      )
    )
  
  # load 2006 speciation file
  load(
    file.path(
      "pre_travis_intermediate_data", 
      "daily_SPEC_2006.rdata"
      )
    )
  
  # create the first edition of the big speciation tibble by a row bind
  raw_speciation_tibble <- 
    bind_rows(
      daily_SPEC_2005, 
      daily_SPEC_2006
    )
  
  # remove the 2005 and 2006 tibbles from memory
  rm(
    daily_SPEC_2005, 
    daily_SPEC_2006)
  invisible(gc())
  
  # make a function to bind the rest of the speciation tibbles
  speciebinder <- 
    function(speciation_filename) {
      
      # make variable of tibble name
      speciation_tibble_name <- 
        str_replace(
          speciation_filename, 
          ".rdata",
          ""
          )
      
      # load next speciation file
      load(
        file.path(
          "pre_travis_intermediate_data", 
          speciation_filename
        )
      )
      
      # bind the speciation file to the big tibble
      raw_speciation_tibble <<- 
        bind_rows(  
          raw_speciation_tibble, 
          eval(as.symbol(speciation_tibble_name)))
      
      # remove the speciation file and clear the memory
      rm(speciation_filename, 
         list = speciation_tibble_name)
      invisible(gc())
      
    }
  
  # make the big speciation dataset
  speciebinder("daily_SPEC_2007.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2008.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2009.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2010.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2011.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2012.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2013.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2014.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2015.rdata")
  invisible(gc())
  
  # save the big speciation dataset
  save(
    raw_speciation_tibble, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "raw_speciation_tibble.rdata"
        )
  )
  
  # remove non-needed files from memory
  rm(speciation_files, 
     speciation_filenames_except_2005_2006,
     raw_speciation_tibble)
  invisible(gc())
  }

```


### air pollution in one big tibble

```{r join pm25 total and speciation data}

if(
  !file.exists(  # checks if tibble already exists
    file.path(
      "pre_travis_intermediate_data",
      "raw_air_tibble.rdata"
      )
    )
  ) {

  # load the total pm2.5 tibble
  load(
    file.path(
      "pre_travis_intermediate_data",
      "raw_pm_25_frm_tibble.rdata"
    )
  )

  # load the pm2.5 speciation tibble
  load(
    file.path(
      "pre_travis_intermediate_data",
      "raw_speciation_tibble.rdata"
    )
  )

  # and in the darkness bind them
  raw_air_tibble <-
    bind_rows(
      raw_pm_25_frm_tibble,
      raw_speciation_tibble)

  # remove extraneous files
  rm(raw_pm_25_frm_tibble,
     raw_speciation_tibble)
  invisible(gc())

  # save the big tibble
  save(
    raw_air_tibble,
    file =
      file.path(
        "pre_travis_intermediate_data",
        "raw_air_tibble.rdata"
      ))
  
  # remove it from memory
  rm(raw_air_tibble)
  invisible(gc())

}

```


### clean and shrink the air pollution data

```{r clean big air pollution tibble}

# LOOK AT ORGANIC AND ELEMENTAL CARBON AGAIN

# load the big air tibble into memory
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "raw_air_tibble.rdata"
    )
)

# make a dataset for cleaning
working_air_tibble <- 
  raw_air_tibble

# # remove the raw air tibble from memory
# rm(raw_air_tibble)
# invisible(gc())

# # get variable names
# names(working_air_tibble)

# # get rows with unique state codes
# states_and_codes <- 
#   working_air_tibble %>% 
#   select(state_name, state_code) %>% 
#   unique()

# list of non_us codes
non_us_state_codes <- 
  c(
    # "72",  # PR (now including)
    "78",  # Virgin Islands (alas, no RUCA/census data)
    "CC",  # Canada
    "80"   # Mexico
    )

# list of parameter names and codes
species_and_codes_from_data <- 
  working_air_tibble %>% 
  select(parameter_name, parameter_code) %>% 
  unique() %>% 
  arrange(parameter_code)

# get observation counts
observations_counts_by_code <- 
  working_air_tibble %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(parameter_code)

# # get organic carbon codes
# organic_carbon_codes <- 
#   species_and_codes_from_data %>% 
#   filter(str_detect(parameter_name, "OC")) %>% 
#   pull(parameter_code)

# get OC/EC fraction codes
fraction_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "EC[0-9]") | 
           str_detect(parameter_name, "OC[0-9]")) %>% 
  pull(parameter_code)

# # get unadjusted codes
# unadjusted_codes <- 
#   species_and_codes_from_data %>% 
#   filter(str_detect(parameter_name, "nadjusted")) %>% 
#   pull(parameter_code)
  
# get OP carbon codes
op_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "OP")) %>% 
  pull(parameter_code)

# get codes for volatile/non-volatile nitrate
volatile_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "olatile")) %>% 
  pull(parameter_code)

# other codes to drop
remaining_drop_codes <- 
  c(
    88308,  # carbonate carbon
    88312,  # total carbon (Total Carbon PM2.5 LC TOT)
    88313,  # black carbon (Black Carbon PM2.5 at 880 nm)
    88314 #,   # UV carbon (UV Carbon PM2.5 at 370 nm)
    # 88316  # optical EC (Optical EC PM2.5 LC TOT)
  )

# remove non_states to make tibble of all monitors with any data in US
us_raw_air_tibble <- 
  working_air_tibble %>% 
  filter(!(state_code %in% non_us_state_codes))  # removes non-US sites

# make monitor id and monitor id with poc
us_raw_air_tibble <- 
  us_raw_air_tibble %>% 
  mutate(
    monitor_id = 
      paste(
        state_code, 
        county_code, 
        site_num, 
        sep="-"), 
    poc = str_pad(poc, 2, pad = "0"),
    monitor_id_poc = str_c(monitor_id, "-", poc)
    )

# save the plain us tibble
save(
  us_raw_air_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_raw_air_tibble.rdata"
  )
)

# remove the working air tibble
rm(working_air_tibble)
invisible(gc())

# drop observations not of interest
us_filtered_air_tibble <- 
  us_raw_air_tibble %>%  
  filter(parameter_code > 88100) %>%   # removes non-parameter variables
  filter(!(parameter_code %in% fraction_codes)) %>%  # remove fractions
  filter(!(parameter_code %in% op_codes)) %>%  # remove pyrolized carbon
  filter(!(parameter_code %in% volatile_codes)) %>%  # remove weird nitrate
#  filter(!(parameter_code %in% unadjusted_codes)) %>%  # remove unadjusteds
  filter(!(parameter_code %in% remaining_drop_codes))  # remove remaining drops

# remove the us raw tibble
rm(us_raw_air_tibble)
invisible(gc())
  
# stuff from data two
species_and_codes_from_data_two <- 
  us_filtered_air_tibble %>% 
  select(parameter_name, parameter_code) %>% 
  unique() %>% 
  arrange(parameter_code)

# count observations by code
observations_counts_by_code <- 
  us_filtered_air_tibble %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(parameter_code)

# save the us filtered air tibble
save(
  us_filtered_air_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_filtered_air_tibble.rdata"
    )
  )

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


```{r split air tibble into travis-y pieces}

# load big filtered air tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_filtered_air_tibble.rdata"
    )
)


# air pollutants only tibble
air_pollutant_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    monitor_id, 
    poc, 
    monitor_id_poc,
    parameter_code,
    sample_duration,
    date_local, 
    event_type, 
    observation_count, 
    observation_percent, 
    arithmetic_mean, 
    `1st_max_value`,
    `1st_max_hour`, 
    method_code
  )

save(
  air_pollutant_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_pollutant_tibble.rdata"
    )
)

rm(air_pollutant_tibble)
invisible(gc())


# method_tibble
method_tibble <-
  us_filtered_air_tibble %>% 
  select(parameter_code, 
         method_code,
         method_name) %>% 
  distinct() %>% 
  arrange(parameter_code, 
          method_code)

save(
  method_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "method_tibble.rdata"
    )
)

rm(method_tibble)
invisible(gc())


# parameter names and codes tibble
parameter_names_codes_tibble <- 
  us_filtered_air_tibble %>% 
  select(parameter_code, 
         parameter_name) %>% 
  distinct() %>% 
  arrange(parameter_code)

save(
  parameter_names_codes_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "parameter_names_codes_tibble.rdata"
    )
)

rm(parameter_names_codes_tibble)
invisible(gc())
  

# units of measure tibble
units_of_measure_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    parameter_code, 
    units_of_measure
  ) %>% 
  distinct()

save(
  units_of_measure_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "units_of_measure_tibble.rdata"
    )
)

rm(units_of_measure_tibble)
invisible(gc())


# monitor tibble
air_monitor_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    monitor_id,
    state_code,
    county_code, 
    site_num, 
    latitude, 
    longitude, 
    datum, 
    local_site_name, 
    address, 
    state_name, 
    county_name, 
    city_name, 
    cbsa_name) %>% 
  distinct()

save(
  air_monitor_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_monitor_tibble.rdata"
    )
)

rm(air_monitor_tibble)
invisible(gc())


# total pm25 88101 tibble with 88101-only variables (pollutant standard, aqi)
total_88101_pollutant_standard_aqi_tibble <- 
  us_filtered_air_tibble %>% 
  filter(parameter_code == 88101) %>% 
  select(
    monitor_id, 
    poc, 
    parameter_code, 
    pollutant_standard, 
    date_local, 
    sample_duration,
    date_local, 
    event_type, 
    observation_count, 
    observation_percent, 
    arithmetic_mean, 
    `1st_max_value`,
    `1st_max_hour`, 
    aqi, 
    method_code
  )

save(
  total_88101_pollutant_standard_aqi_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```


## travis cleanup

```{r save travis files in cheating folder}

# save needed files from the work above to memory_savers

# state abbreviation character vector
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "state_abbreviation_character_vector.rdata"
    ) 
)

save(
  state_abbreviation_character_vector, 
  file = 
    file.path(
      "travis_cache", 
      "state_abbreviation_character_vector.rdata"
    )
)

rm(state_abbreviation_character_vector)
invisible(gc())


# full shapefiles
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "tract_shapefiles.rdata"
    )
)

save(
  tract_shapefiles, 
  file = 
    file.path(
      "travis_cache", 
      "tract_shapefiles.rdata"
    )
)

rm(tract_shapefiles)
invisible(gc())


# cartographic boundary files
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "tract_cbfs.rdata"
    )
)

save(
  tract_cbfs, 
  file = 
    file.path(
      "travis_cache", 
      "tract_cbfs.rdata"
    )
)

rm(tract_cbfs)
invisible(gc())


# shapefile-based neighbor files
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "neighbor_tibble_long.rdata"
    )
)

save(
  neighbor_tibble_long, 
  file = 
    file.path(
      "travis_cache", 
      "neighbor_tibble_long.rdata"
    )
  )

rm(neighbor_tibble_long)
invisible(gc())  


# cbf-based neighbor files
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "cbf_neighbor_tibble_long.rdata"
    )
)

save(
  cbf_neighbor_tibble_long, 
  file = 
    file.path(
      "travis_cache", 
      "cbf_neighbor_tibble_long.rdata"
    )
  )

rm(cbf_neighbor_tibble_long)
invisible(gc())  


# sf1 census 2010 data
load(
  file = 
    file.path(
      "raw_data", 
      "sf1_totalcensus_raw_dataframe.rdata"
      )
  )

save(
  sf1_totalcensus_raw_dataframe,
  file = 
    file.path(
      "travis_cache", 
      "sf1_totalcensus_raw_dataframe.rdata"
      )
  )

rm(sf1_totalcensus_raw_dataframe)
invisible(gc())


# acs 2012 data
load(
  file = 
    file.path(
      "raw_data", 
      "acs_totalcensus_raw_dataframe.rdata"
    )
)

save(
  acs_totalcensus_raw_dataframe,
  file = 
    file.path(
      "travis_cache", 
      "acs_totalcensus_raw_dataframe.rdata"
      )
  )

rm(acs_totalcensus_raw_dataframe)
invisible(gc())


# historical income tibble
load(
  file =
    file.path(
      "raw_data", 
      "inc_hh_hist_raw.rdata"
    )
)

save(
  inc_hh_hist_raw,
  file =
    file.path(
      "travis_cache", 
      "inc_hh_hist_raw.rdata"
    )
)

rm(inc_hh_hist_raw)
invisible(gc())


# ruca files
load(
  file =
    file.path(
      "raw_data",
      "ruca_2010_raw.rdata"
    )
  )

save(
  ruca_2010_raw,
  file =
    file.path(
      "travis_cache",
      "ruca_2010_raw.rdata"
    )
  )

rm(ruca_2010_raw)
invisible(gc())


# air pollutant only tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_pollutant_tibble.rdata"
    )
)

save(
  air_pollutant_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "air_pollutant_tibble.rdata"
    )
)

rm(air_pollutant_tibble)
invisible(gc())


# air monitor only tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_monitor_tibble.rdata"
    )
)

save(
  air_monitor_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "air_monitor_tibble.rdata"
    )
)

rm(air_monitor_tibble)
invisible(gc())


# method_tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "method_tibble.rdata"
    )
)

save(
  method_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "method_tibble.rdata"
    )
)

rm(method_tibble)
invisible(gc())


# parameter names and codes tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "parameter_names_codes_tibble.rdata"
    )
)

save(
  parameter_names_codes_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "parameter_names_codes_tibble.rdata"
    )
)

rm(parameter_names_codes_tibble)
invisible(gc())
  

# units of measure tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "units_of_measure_tibble.rdata"
    )
)

save(
  units_of_measure_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "units_of_measure_tibble.rdata"
    )
)

rm(units_of_measure_tibble)
invisible(gc())


# total pm25 88101 tibble with 88101-only variables (pollutant standard, aqi)
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

save(
  total_88101_pollutant_standard_aqi_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

# remove all objects from the environment
rm(list = ls(all.names = TRUE))

```

