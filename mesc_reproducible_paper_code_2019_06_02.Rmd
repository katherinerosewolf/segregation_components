---
title: "Segregation Components Paper Code"
author: "Katherine Rose Wolf"
date: "June 12, 2019"
output: html_document
---


# CODE SETUP (run every time)

```{r rmarkdown setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load packages}

library(rmarkdown)
library(knitr)
library(plyr)
library(data.table)
library(Hmisc)
library(extrafontdb)
library(extrafont)
library(NADA)
library(grid)
library(gridExtra)
library(lattice)
library(tmap)
library(sf)
library(tidycensus)
library(tigris)
library(rgeos)
library(sjPlot)
library(stargazer)
library(readxl)
library(tidyverse)
library(totalcensus)
library(spdep)
library(lubridate)

```


```{r establish directory structure}

if(!dir.exists("raw_data")) {
  dir.create("raw_data")
}

if(!dir.exists(file.path("raw_data", "totalcensus"))) {
  dir.create(file.path("raw_data", "totalcensus"))
}

if(!dir.exists("pre_travis_intermediate_data")) {
  dir.create("pre_travis_intermediate_data")
}

if(!dir.exists("travis_cache")) {
  dir.create("travis_cache")
}

if(!dir.exists("intermediate_data")) {
  dir.create("intermediate_data")
}

if(!dir.exists(file.path("intermediate_data", "plots"))) {
  dir.create(file.path("intermediate_data", "plots"))
}

if(!dir.exists(file.path("functions"))) {
  dir.create(file.path("functions"))
}

```


```{r census api key and options for tidycensus and totalcensus}

# install the census api key for tidycensus
census_api_key("eae31d12c8d9c2c9ee288e096e0c03830744aaef", 
               install = TRUE, 
               overwrite = TRUE)

readRenviron("~/.Renviron")

# have tidycensus store old shapefiles for future use
options(tigris_use_cache = TRUE)

# set path to census data for totalcensus
Sys.setenv(PATH_TO_CENSUS = "raw_data/totalcensus")

```


```{r functions for general later use}

# function to convert column names to snake case
column_name_fixer <-
  function(tibble) {
  new_names <-
    tibble %>%
    colnames() %>%  # gets the column names from the tibble
    tolower() %>%  # converts the column names to lowercase
    {gsub(" ", "_", .)}  # replaces spaces with "_"
  colnames(tibble) <-
    new_names  # rename the columns in the original tibble
  return(tibble)
  }

# create function that downloads files
file_downloader <-
  function(desired_filename_as_string, 
           web_address_as_string, 
           folder_as_string) {
    
    path <-  # specify the file path
      file.path(
        folder_as_string,
        desired_filename_as_string) 
    
    if(!file.exists(path))  # checks if file exists already
      download.file(url = web_address_as_string,  # if not, downloads/saves file
                    destfile = path, 
                    mode = "wb")
  }

# function to download generated data without asking
download_generated_data_auto <- 
  function() {
    cat(paste("Downloading data generated from decennial census 2010."))
    total_files <- 426
    path_to_census <- Sys.getenv("PATH_TO_CENSUS")
    url <- "https://s3.amazonaws.com/gl-shared-data/generated_census_data_v060.zip"
    download.file(url, paste0(path_to_census, "/tmp.zip"))
    unzip(paste0(path_to_census, "/tmp.zip"), exdir = paste0(path_to_census, 
        "/generated_data"))
    file.remove(paste0(path_to_census, "/tmp.zip"))
    n_files <- length(list.files(paste0(path_to_census, "/generated_data"), 
        recursive = TRUE))
    if (n_files == total_files) {
        cat("Extraction is successful!\n\n")
    }
    else {
        cat("Last downloading or extraction has a problem. Download and extract again.")
        download_generated_data_auto()
    }
  }

# save functions for later use
save(
  column_name_fixer, 
     file = 
       file.path(
         "functions", 
         "column_name_fixer.rdata"
       )
)

save(
  file_downloader, 
     file = 
       file.path(
         "functions", 
         "file_downloader.rdata"
       )
)

save(
  download_generated_data_auto, 
     file = 
       file.path(
         "functions", 
         "download_generated_data_auto.rdata"
       )
)

# remove the functions
rm(
  column_name_fixer, 
  file_downloader, 
  download_generated_data_auto
)
invisible(gc())

```


# TOO BIG FOR TRAVIS

## geography

### states for analyses

```{r make state abbreviation character vector, eval=FALSE, include=FALSE}

# check if file already exists
if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "state_abbreviation_character_vector.rdata"))) {
  
# make vector of state abbreviations (includes PR, no VI)
state_abbreviation_character_vector <- 
  c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA", "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME", "MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM", "NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN", "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")

# save to disk
save(
  state_abbreviation_character_vector, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "state_abbreviation_character_vector.rdata"
    )
)

# remove it
rm(state_abbreviation_character_vector)
invisible(gc())
}

```


### census shapefile without data

```{r census detailed shapefile without data, eval=FALSE, include=FALSE}

# source https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_%s_tract10.zip
# where %s is the state FIPS code
# downloaded 27 June 2019
# last modified 23 June 2011 (state 60, others earlier)

if(
  !file.exists(
    file.path(
      "raw_data", 
      "tigris_shapefiles_no_data.rdata"
    )
    )
  ) {
  
  # load the column name fixer function
  load(
    file = 
      file.path(
       "functions", 
       "column_name_fixer.rdata"
      )
    )
  
  # load vector of state abbreviations
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
  )
  
  # download the data
  tigris_shapefiles_no_data <- 
    rbind_tigris(
    map(
      state_abbreviation_character_vector, 
      function(x) {
        tracts(
          x, 
          cb = FALSE, 
          year = 2010, 
          class = 'sf'
          )
        }
      )
  )
  
  # make into tibble
  tigris_shapefiles_no_data <- 
    tigris_shapefiles_no_data %>% 
    as_tibble() %>% 
    st_as_sf()
  
  # fix column names
  tigris_shapefiles_no_data <- 
    tigris_shapefiles_no_data %>% 
    column_name_fixer()
  
  # save the result to disk
  save(
    tigris_shapefiles_no_data, 
    file = 
      file.path(
        "raw_data", 
        "tigris_shapefiles_no_data.rdata"
      )
  )
  
  # remove loaded and created objects from environment and clear memory
  rm(
    state_abbreviation_character_vector,
    column_name_fixer,
    tigris_shapefiles_no_data 
  )
  invisible(gc())
}

```


```{r make list of tract neighbors for later segregation, eval=FALSE, include=FALSE}

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "neighbor_tibble_long.rdata"
    )
  )
) {

  # load the census tract shapefiles
  load(
    file =
      file.path(
        "raw_data",
        "tigris_shapefiles_no_data.rdata"
      )
    )
  
  # get character vector for numbers from 1 to the number of tracts
  tract_order_character <- 
    as.character(1:nrow(tigris_shapefiles_no_data))
  
  # pull the geoids as a vector
  tigris_geoid_character <- 
    tigris_shapefiles_no_data %>% 
    pull(geoid10)
  
  # make tibble indexing tracts in order to match geoids with the spdep output
  geoid_index_tibble <- 
    tibble(index = tract_order_character, 
           geoid = tigris_geoid_character)
    
  if(!file.exists(file.path("pre_travis_intermediate_data", "tract_neighbor_nb.rdata"))){
    
    # make a list of lists of neighbors to each tract (uses spdep package)
    tract_neighbor_nb <-  
      poly2nb(pl = tigris_shapefiles_no_data, 
              queen = TRUE)
  
    # name the elements of the big list above with their tract indices
    names(tract_neighbor_nb) <- tract_order_character
  
  } else {
    
    # load the file
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "tract_neighbor_nb.rdata"
      )
    )
  }
  
  # check if the neighbors-as-one-string tibble already exists
  if(!file.exists(file.path(
    "pre_travis_intermediate_data", 
    "neighbors_in_one_string_tibble.rdata"))){
    
  # if not, create it
  neighbors_in_one_string_tibble <- 
    map_df(  # makes a tibble from the output of poly2nb()
      tract_neighbor_nb, 
      paste,  # puts all the adjacent tracts into one string
      collapse = ","  # separated by commas
      ) %>%  # makes two columns from one row with column names
    gather(key = "primary_tract",  
           value = "adjacent_tracts") %>% 
    mutate(  # adds column of the primary tract and its neighbors in one string
      primary_and_adjacent_tracts = 
        paste(primary_tract, 
              adjacent_tracts, 
              sep = ",")
    )
  
    # save the result
    save(neighbors_in_one_string_tibble,
           file = file.path("pre_travis_intermediate_data", 
                            "neighbors_in_one_string_tibble.rdata"))
    
  } else {  
    
    # otherwise load the relevant tibble
    load(
      file.path(
        "pre_travis_intermediate_data", 
        "neighbors_in_one_string_tibble.rdata"
      )
    )
  }
  
  # make list of character vectors of neighbor tract indices
  # ordered by primary tract index
  split_tracts <- 
    neighbors_in_one_string_tibble %>% 
    pull(primary_and_adjacent_tracts) %>%  # pull the self and neighbor column
    str_split(pattern = ",")  # split strings of neighbor indices
  
  # name each list element by its primary tract
  names(split_tracts) <- 
    neighbors_in_one_string_tibble$primary_tract
  
  # make long tibble repeating the primary tract for itself and every neighbor
  neighbor_tibble_long <- 
    as_tibble(  # makes it a tibble repeating the primary (index) for each neighbor
      stack(split_tracts)[2:1]) %>%  # reorders them so that the primary tract comes first
    rename(primary_tract = 1,  # renames the columns with meaningful names
           neighbor_tract = 2) %>% 
    mutate(  # coerce primary tract column (index) back to character
      primary_tract = as.character(primary_tract)
      ) %>% 
    left_join(
      geoid_index_tibble,  # join with the original geoids corresponding
      by = c("primary_tract" = "index")  # to the primary tract
      ) %>%  
    rename(geoid_primary_tract = geoid) %>%  # rename geoid for primary tract
    left_join(
      geoid_index_tibble,  # join with the original geoids corresponding
      by = c("neighbor_tract" = "index")  # to the neighbor tract
      ) %>%  
    rename(geoid_neighbor_tract = geoid) # rename geoid for neighbor tract
  
  save(
    neighbor_tibble_long, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "neighbor_tibble_long.rdata"
      )
    )
  
  # remove all the files
  rm(
    tigris_shapefiles_no_data, 
    tract_neighbor_nb,
    tract_order_character, 
    tigris_geoid_character,
    geoid_index_tibble,
    neighbors_in_one_string_tibble,
    split_tracts,
    neighbor_tibble_long
  )
  invisible(gc())
}

```


## demographic and ses data

### census

#### census variable lists

```{r census variable lists, eval=FALSE, include=FALSE}

# check if variable list already exists
if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "sf1_totalcensus_var_vec.rdata"))) {

  # # view census variables
  # census_variables_sf1 <- 
  #   load_variables(
  #     2010, 
  #     "sf1", 
  #     cache = TRUE
  #     )
  # census_variables_sf3 <- 
  #   load_variables(
  #     2000, 
  #     "sf3", 
  #     cache = TRUE
  #     )
  # acs_variables <- 
  #   load_variables(
  #     2012, 
  #     "acs", 
  #     cache = TRUE
  #   )
  
  # define categories for variables
  
  # total population (SF1 P1)
  # universe: total population
  total_pop_sf1_p1 <-
    c("P0010001")
  
  # hispanic or latino origin by race (SF1 P5)
  # universe: total population
  race_sf1_p5 <-
    sprintf("P0050%0.3d", 1:17)
  
  # P0050001  Total population:
  # P0050002    Not Hispanic or Latino:
  # P0050003      White alone
  # P0050004      Black or African American alone
  # P0050005      American Indian and Alaska Native alone
  # P0050006      Asian alone
  # P0050007      Native Hawaiian and Other Pacific Islander alone
  # P0050008      Some Other Race alone
  # P0050009      Two or More Races
  # P0050010    Hispanic or Latino:
  # P0050011      White alone
  # P0050012      Black or African American alone
  # P0050013      American Indian and Alaska Native alone
  # P0050014      Asian alone
  # P0050015      Native Hawaiian and Other Pacific Islander alone
  # P0050016      Some Other Race alone
  # P0050017      Two or More Races
  
  # race (SF1 PCT23)
  # Universe: Total population
  race_sf1_pct23 <- 
    sprintf("PCT023%0.4d", 1:24)
  
  # PCT0230001  Total: 
  # PCT0230002  Population of one race: 
  # PCT0230003    White 
  # PCT0230004    Black or African American 
  # PCT0230005    American Indian and Alaska Native 
  # PCT0230006    Asian: 
  # PCT0230007      Asian Indian 
  # PCT0230008      Chinese (including Taiwanese) 
  # PCT0230009      Filipino 
  # PCT0230010      Japanese 
  # PCT0230011      Korean 
  # PCT0230012      Vietnamese 
  # PCT0230013      Other Asian 
  # PCT0230014    Native Hawaiian and Other Pacific Islander: 
  # PCT0230015      Native Hawaiian 
  # PCT0230016      Guamanian or Chamorro 
  # PCT0230017      Samoan 
  # PCT0230018      Other Pacific Islander 
  # PCT0230019    Some Other Race 
  # PCT0230020    Population of Two or More Races 
  # PCT0230021      White; American Indian and Alaska Native 
  # PCT0230022      White; Asian 
  # PCT0230023      White; Black or African American 
  # PCT0230024      White; Some Other Race 
  
  # hispanic or latino by specific orgiin (SF1, PCT11) [31]
  # Universe: Total population
  hisp_lat_origin_sf1_pct11 <- 
    sprintf("PCT011%0.4d", 1:31)
  
  # PCT0110001  Total:
  # PCT0110002    Not Hispanic or Latino (001–199, 300–999) 
  # PCT0110003    Hispanic or Latino (200–299): 
  # PCT0110004      Mexican (210–220) 
  # PCT0110005      Puerto Rican (260–269) 
  # PCT0110006      Cuban (270–274) 
  # PCT0110007      Dominican (275–279) 
  # PCT0110008      Central American (excluding Mexican) (221–230): 
  # PCT0110009        Costa Rican (221) 
  # PCT0110010        Guatemalan (222) 
  # PCT0110011        Honduran (223) 
  # PCT0110012        Nicaraguan (224) 
  # PCT0110013        Panamanian (225) 
  # PCT0110014        Salvadoran (226) 
  # PCT0110015        Other Central American (227–230) 
  # PCT0110016      South American (231–249): 
  # PCT0110017        Argentinean (231) 
  # PCT0110018        Bolivian (232) 
  # PCT0110019        Chilean (233) 
  # PCT0110020        Colombian (234) 
  # PCT0110021        Ecuadorian (235) 
  # PCT0110022        Paraguayan (236) 
  # PCT0110023        Peruvian (237) 
  # PCT0110024        Uruguayan (238) 
  # PCT0110025        Venezuelan (239) 
  # PCT0110026        Other South American (240–249) 
  # PCT0110027      Other Hispanic or Latino (200–209, 250–259, 280–299): 
  # PCT0110028        Spaniard (200–209) 
  # PCT0110029        Spanish (282) 
  # PCT0110030        Spanish American (286) 
  # PCT0110031        All other Hispanic or Latino (250–259, 280–281, 283–285, 287–299) 
  
  
  # sex by age (SF1 P12)
  # universe: total population
  sex_by_age_sf1_p12 <- 
    sprintf("P0120%0.3d", 1:49)
  # NOTE: P0120002 is all male and P0120026 is all female
  
  # P0120001  Total population:
  # P0120002    Male:
  # P0120003      Under 5 years
  # P0120004      5 to 9 years
  # P0120005      10 to 14 years
  # P0120006      15 to 17 years
  # P0120007      18 and 19 years
  # P0120008      20 years
  # P0120009      21 years
  # P0120010      22 to 24 years
  # P0120011      25 to 29 years
  # P0120012      30 to 34 years
  # P0120013      35 to 39 years
  # P0120014      40 to 44 years
  # P0120015      45 to 49 years
  # P0120016      50 to 54 years
  # P0120017      55 to 59 years
  # P0120018      60 and 61 years
  # P0120019      62 to 64 years
  # P0120020      65 and 66 years
  # P0120021      67 to 69 years
  # P0120022      70 to 74 years
  # P0120023      75 to 79 years
  # P0120024      80 to 84 years
  # P0120025      85 years and over
  # P0120026    Female:
  # P0120027      Under 5 years
  # P0120028      5 to 9 years
  # P0120029      10 to 14 years
  # P0120030      15 to 17 years
  # P0120031      18 and 19 years
  # P0120032      20 years
  # P0120033      21 years
  # P0120034      22 to 24 years
  # P0120035      25 to 29 years
  # P0120036      30 to 34 years
  # P0120037      35 to 39 years
  # P0120038      40 to 44 years
  # P0120039      45 to 49 years
  # P0120040      50 to 54 years
  # P0120041      55 to 59 years
  # P0120042      60 and 61 years
  # P0120043      62 to 64 years
  # P0120044      65 and 66 years
  # P0120045      67 to 69 years
  # P0120046      70 to 74 years
  # P0120047      75 to 79 years
  # P0120048      80 to 84 years
  # P0120049      85 years and over
  
  # median age (SF1 P13)
  # universe: total population
  age_median_sf1_p13 <- 
    "P0130001"
  
  # occupancy status
  # universe: housing units
  # 1 total, 2 occupied, 3 vacant
  occupancy_sf1_h3 <-
    sprintf("H00300%0.2d", 1:3)
  
  # housing tenure (SF1 H4)
  # universe: occupied housing units
  # 1 total, 2 owned w/ a mortgage or loan, 3 owned free and clear, 4 renter-occupied
  housing_tenure_sf1_h4 <-  
    sprintf("H00400%0.2d", 1:4)
  
  # population in occupied housing units by tenure
  # universe: population in occupied housing units
  housing_tenure_people_sf1_h11 <- 
    c("H0110001",  # total population in occupied housing units
      "H0110002",  # owned with a mortgage or a loan 
      "H0110003",  # owned free and clear
      "H0110004")  # renter-occupied
  
  # make list of needed variables
  sf1_totalcensus_var_list <- 
    list(
      total_pop_sf1_p1,
      race_sf1_p5,
      race_sf1_pct23, 
      hisp_lat_origin_sf1_pct11,
      sex_by_age_sf1_p12,
      age_median_sf1_p13,
      occupancy_sf1_h3,
      housing_tenure_sf1_h4,
      housing_tenure_people_sf1_h11
    )
  
  # make big master variable list
  sf1_totalcensus_var_vec <- 
    sf1_totalcensus_var_list %>% 
    reduce(c)
  
  # save the master variable list
  save(
    sf1_totalcensus_var_vec, 
    file = 
      file.path("pre_travis_intermediate_data", 
                "sf1_totalcensus_var_vec.rdata")
  )
  
  rm(
    total_pop_sf1_p1,
    race_sf1_p5,
    sex_by_age_sf1_p12,
    age_median_sf1_p13,
    occupancy_sf1_h3,
    housing_tenure_sf1_h4,
    housing_tenure_people_sf1_h11, 
    sf1_totalcensus_var_list, 
    sf1_totalcensus_var_vec
    )
  invisible(gc())
}

```


#### census totalcensus variable pulls

```{r sf1 totalcensus data download, eval=FALSE, include=FALSE}

# header information from
# file source: https://www2.census.gov/census_2010/04-Summary_File_1/Urban_Rural_Update/
# individual state files from there
# downloaded 2019-06-27
# last updated per website 2012-09-27

# check if the raw sf1 totalcensus tibble exists and if so, skip the whole chunk
if(!
   file.exists(
     file.path(
       "raw_data", 
       "sf1_totalcensus_raw_dataframe.rdata"
     )
   )
   ) {
  
  # load state abbreviation character vector
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
  )
  
  # load census variable names
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "sf1_totalcensus_var_vec.rdata")
  )
  
  # download the data necessary to use the census data if it isn't already there
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "generated_data"
        )
      )
    ) 
  download_generated_data_auto()
  
  # check if 2010 census data is there
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "census2010"
        )
      )
    ) {
    
    # download the census data if it's not
      download_census(
        survey = "dec",
        year = 2010, 
        states = state_abbreviation_character_vector
        )
  }
  
  # create dataframe of needed variables
  sf1_totalcensus_raw_dataframe <-
    read_decennial(
      year = 2010,
      states = state_abbreviation_character_vector,
      table_contents =
        sf1_totalcensus_var_vec,
      summary_level = "tract"
  )
  
  # save the tibble to disk
  save(
    sf1_totalcensus_raw_dataframe, 
    file = 
      file.path(
        "raw_data",
        "sf1_totalcensus_raw_dataframe.rdata"
        )
    )
  
  # remove working files
  rm(
    sf1_totalcensus_var_vec,
    state_abbreviation_character_vector, 
    sf1_totalcensus_raw_dataframe
    )
  invisible(gc())
}

```


### acs

#### acs variable lists

```{r acs totalcensus variable lists, eval=FALSE, include=FALSE}

if(!file.exists(file.path("pre_travis_intermediate_data", 
                          "acs_totalcensus_var_vec.rdata"))) {

  # educational attainment for the population 25 years and over
  # universe: population 25 years and over
  edu_cat_b15003 <- 
    c("B15003_001",  # total
      "B15003_002",  # no schooling completed
      "B15003_003",  # nursery school
      "B15003_004",  # kindergarten
      "B15003_005",  # 1st grade
      "B15003_006",  # 2nd grade
      "B15003_007",  # 3rd grade
      "B15003_008",  # 4th grade
      "B15003_009",  # 5th grade
      "B15003_010",  # 6th grade
      "B15003_011",  # 7th grade
      "B15003_012",  # 8th grade
      "B15003_013",  # 9th grade
      "B15003_014",  # 10th grade
      "B15003_015",  # 11th grade
      "B15003_016",  # 12th grade
      "B15003_017",  # regular high school diploma
      "B15003_018",  # GED or alternative credential
      "B15003_019",  # some college, less than 1 year
      "B15003_020",  # some college, 1 or more years, no degree
      "B15003_021",  # associate's degree
      "B15003_022",  # bachelor's degree
      "B15003_023",  # master's degree
      "B15003_024",  # professional school degree
      "B15003_025")  # doctorate degree
  
  # ratio of income to poverty level in the past 12 months
  # universe: population for whom poverty status is determined
  poverty_ratio_cat_c17002 <-
    c("C17002_001",  # total
      "C17002_002",  # under .50
      "C17002_003",  # .50 to .99
      "C17002_004",  # 1.00 to 1.24
      "C17002_005",  # 1.25 to 1.49
      "C17002_006",  # 1.50 to 1.84
      "C17002_007",  # 1.85 to 1.99
      "C17002_008")  # 2.00 and over
  
  # household income in the past 12 months (in 2012 inflation-adjusted dollars)
  # universe: households
  income_house_cat_b19001 <-
    c("B19001_001",  # total
      "B19001_002",  # less than $10,000
      "B19001_003",  # $10,000 to $14,999
      "B19001_004",  # $15,000 to $19,999
      "B19001_005",  # $20,000 to $24,999
      "B19001_006",  # $25,000 to $29,999
      "B19001_007",  # $30,000 to $34,999
      "B19001_008",  # $35,000 to $39,999
      "B19001_009",  # $40,000 to $44,999
      "B19001_010",  # $45,000 to $49,999
      "B19001_011",  # $50,000 to $59,999
      "B19001_012",  # $60,000 to $74,999
      "B19001_013",  # $75,000 to $99,999
      "B19001_014",  # $100,000 to $124,999
      "B19001_015",  # $125,000 to $149,999
      "B19001_016",  # $150,000 to $199,999
      "B19001_017")  # $200,000 or more
  
  # median household income in the past 12 months (in 2012 inflation-adjusted dollars)
  # universe: households
  income_house_median_b19013 <-
    c("B19013_001")
  
  # sex by earnings in the past 12 months (in 2012 inflation-adjusted dollars) for the population 16 years and over with earnings in the past 12 months
  # universe: population 16 years and over with earnings
  earnings_sex_cat_b20001 <-
    c("B20001_001",  # total
      "B20001_002",  # male
      "B20001_003",  # $1 to $2,499 or loss
      "B20001_004",  # $2,500 to $4,999
      "B20001_005",  # $5,000 to $7,499
      "B20001_006",  # $7,500 to $9,999
      "B20001_007",  # $10,000 to $12,499
      "B20001_008",  # $12,500 to $14,999
      "B20001_009",  # $15,000 to $17,499
      "B20001_010",  # $17,500 to $19,999
      "B20001_011",  # $20,000 to $22,499
      "B20001_012",  # $22,500 to $24,999
      "B20001_013",  # $25,000 to $29,999
      "B20001_014",  # $30,000 to $34,999
      "B20001_015",  # $35,000 to $39,999
      "B20001_016",  # $40,000 to $44,999
      "B20001_017",  # $45,000 to $49,999
      "B20001_018",  # $50,000 to $54,999
      "B20001_019",  # $55,000 to $64,999
      "B20001_020",  # $65,000 to $74,999
      "B20001_021",  # $75,000 to $99,999
      "B20001_022",  # $100,000 or more
      "B20001_023",  # Female:
      "B20001_024",  # $1 to $2,499 or loss
      "B20001_025",  # $2,500 to $4,999
      "B20001_026",  # $5,000 to $7,499
      "B20001_027",  # $7,500 to $9,999
      "B20001_028",  # $10,000 to $12,499
      "B20001_029",  # $12,500 to $14,999
      "B20001_030",  # $15,000 to $17,499
      "B20001_031",  # $17,500 to $19,999
      "B20001_032",  # $20,000 to $22,499
      "B20001_033",  # $22,500 to $24,999
      "B20001_034",  # $25,000 to $29,999
      "B20001_035",  # $30,000 to $34,999
      "B20001_036",  # $35,000 to $39,999
      "B20001_037",  # $40,000 to $44,999
      "B20001_038",  # $45,000 to $49,999
      "B20001_039",  # $50,000 to $54,999
      "B20001_040",  # $55,000 to $64,999
      "B20001_041",  # $65,000 to $74,999
      "B20001_042",  # $75,000 to $99,999
      "B20001_043")  # $100,000 or more
  
  # median earnings in the past 12 months (in 2012 inflation-adjusted dollars) by sex for the population 16 years and over with earnings in the past 12 months
  # universe: population 16 years and over with earnings
  earnings_median_b20002 <-
    c("B20002_001",  # total
      "B20002_002",  # male
      "B20002_003")  # female
  
  # employment status for the population 16 years and over
  # universe: population 16 years and over
  employment_cat_b23025 <-  # divide 5 by 3
    c("B23025_001",  # total
      "B23025_002",    # in labor force
      "B23025_003",      # civilian labor force
      "B23025_004",        # employed
      "B23025_005",        # unemployed
      "B23025_006",      # armed forces
      "B23025_007")    # not in labor force
    
  # value
  # universe: owner-occupied housing units
  home_value_cat_b25075 <-
    c("B25075_001",  # total
      "B25075_002",  # less than $10,000
      "B25075_003",  # $10,000 to $14,999
      "B25075_004",  # $15,000 to $19,999
      "B25075_005",  # $20,000 to $24,999
      "B25075_006",  # $25,000 to $29,999
      "B25075_007",  # $30,000 to $34,999
      "B25075_008",  # $35,000 to $39,999
      "B25075_009",  # $40,000 to $49,999
      "B25075_010",  # $50,000 to $59,999
      "B25075_011",  # $60,000 to $69,999
      "B25075_012",  # $70,000 to $79,999
      "B25075_013",  # $80,000 to $89,999
      "B25075_014",  # $90,000 to $99,999
      "B25075_015",  # $100,000 to $124,999
      "B25075_016",  # $125,000 to $149,999
      "B25075_017",  # $150,000 to $174,999
      "B25075_018",  # $175,000 to $199,999
      "B25075_019",  # $200,000 to $249,999
      "B25075_020",  # $250,000 to $299,999
      "B25075_021",  # $300,000 to $399,999
      "B25075_022",  # $400,000 to $499,999
      "B25075_023",  # $500,000 to $749,999
      "B25075_024",  # $750,000 to $999,999
      "B25075_025")  # $1,000,000 or more
  
  # median value (dollars)
  # universe: owner-occupied housing units
  home_value_median_b25077 <-
    c("B25077_001")  # median value (dollars)
  
  # types of health insurance coverage by age
  # universe: civilian noninstitutionalized population
  health_insurance_cat_c27010 <-
    c("C27010_001",  # total
      "C27010_002",    # under 18 years
      "C27010_003",      # with private health insurance only
      "C27010_004",      # with public coverage only
      "C27010_005",      # with both private and public coverage
      "C27010_006",      # no health insurance coverage
      "C27010_007",    # 18 to 34 years
      "C27010_008",      # with private health insurance only
      "C27010_009",      # with public coverage only
      "C27010_010",      # with both private and public coverage
      "C27010_011",      # no health insurance coverage
      "C27010_012",    # 35 to 64 years
      "C27010_013",      # with private health insurance only
      "C27010_014",      # with public coverage only
      "C27010_015",      # with both private and public coverage
      "C27010_016",      # no health insurance coverage
      "C27010_017",    # 65 years and over
      "C27010_018",      # with private health insurance only
      "C27010_019",      # with public coverage only
      "C27010_020",      # with both private and public coverage
      "C27010_021")      # no health insurance coverage
  
  # household language by households in which no one 14 and over speaks english only or speaks a language other than english at home and speaks english "very well"
  # universe: households
  ling_iso_b16002 <-
    c("B16002_001",  # total
      "B16002_002",    # English only
      "B16002_003",    # Spanish:
      "B16002_004",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_005",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_006",    # Other Indo-European languages:
      "B16002_007",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_008",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_009",    # Asian and Pacific Island languages:
      "B16002_010",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_011",      # At least one person 14 and over speaks English only or speaks English "very well"
      "B16002_012",    # Other languages: 
      "B16002_013",      # No one 14 and over speaks English only or speaks English "very well"
      "B16002_014")     # At least one person 14 and over speaks English only or speaks English "very well"
  
  # make list of needed variables
  acs_totalcensus_var_list <- 
    list(
      edu_cat_b15003,
      poverty_ratio_cat_c17002,
      income_house_cat_b19001,
      income_house_median_b19013,
      earnings_sex_cat_b20001,
      earnings_median_b20002,
      employment_cat_b23025,
      home_value_cat_b25075,
      home_value_median_b25077,
      health_insurance_cat_c27010,
      ling_iso_b16002
    )
  
  # make big master variable list
  acs_totalcensus_var_vec <-
    acs_totalcensus_var_list %>% 
    reduce(c)
    
    # save the master variable list
  save(
    acs_totalcensus_var_vec, 
    file = 
      file.path("pre_travis_intermediate_data", 
                "acs_totalcensus_var_vec.rdata")
  )
  
  rm(
    edu_cat_b15003,
    poverty_ratio_cat_c17002,
    income_house_cat_b19001,
    income_house_median_b19013,
    earnings_sex_cat_b20001,
    earnings_median_b20002,
    employment_cat_b23025,
    home_value_cat_b25075,
    home_value_median_b25077,
    health_insurance_cat_c27010,
    ling_iso_b16002,
    acs_totalcensus_var_list, 
    acs_totalcensus_var_vec
    )
  invisible(gc())
}

```

#### acs totalcensus pulls

```{r acs totalcensus data download, eval=FALSE, include=FALSE}

# data downloaded from https://www2.census.gov/programs-surveys/acs/summary_file/2012/data/5_year_by_state/ (then individual state files)
# downloaded 2019-06-27
# last update per website 2015-02-27

# check if the acs totalcensus tibble exists and skip the whole chunk if not
if(!
   file.exists(
     file.path(
       "raw_data", 
       "acs_totalcensus_raw_dataframe.rdata"
     )
   )
   ) {

  # load variable names for reading
  load(
    file = 
      file.path("pre_travis_intermediate_data", 
                "acs_totalcensus_var_vec.rdata")
  )
  
  # load state abbreviation character vector
  load(
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "state_abbreviation_character_vector.rdata"
      )
    )
  
  # download generated data that totalcensus requires if haven't already
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "generated_data"
        )
      )
    )
    download_generated_data_auto()
  
  # download actual acs data if haven't already
  if(
    !dir.exists(
      file.path(
        "raw_data", 
        "totalcensus", 
        "acs5year", 
        "2012"
        )
      )
    ) {
    download_census(
      survey = "acs5", 
      year = 2012, 
      states = "US"
      )
  }
  
  # this downloads the actual data!
  acs_totalcensus_raw_dataframe <-
    read_acs5year(
      year = 2012,
      states = state_abbreviation_character_vector,
      table_contents =
        acs_totalcensus_var_vec,
      summary_level = "tract",
      with_margin = TRUE
  )
  
  # save raw dataframe to disk
  save(
    acs_totalcensus_raw_dataframe, 
    file = 
      file.path(
        "raw_data", 
        "acs_totalcensus_raw_dataframe.rdata"
      )
  )
  
  # remove working files
  rm(
    acs_totalcensus_var_vec,
    state_abbreviation_character_vector, 
    acs_totalcensus_raw_dataframe
  )
  invisible(gc())
}


```


### historical income

```{r historical income table download, eval=FALSE, include=FALSE}

# source https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls
# downloaded 2019-07-09 (ET)
# last updated 2018-08-28 per https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-income-households.html

# check if the historical income file exists and skip the whole chunk if not
if(!
   file.exists(
     file.path(
       "raw_data", 
       "inc_hh_hist_raw.rdata"
     )
   )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # download the historical income file from the internet
  file_downloader(
    desired_filename_as_string = "inc_hh_hist_raw.xls", 
    web_address_as_string = 
      "https://www2.census.gov/programs-surveys/cps/tables/time-series/historical-income-households/h01ar.xls", 
    folder_as_string = "raw_data"
    )
  
  # read the data from the excel file (current dollars only)
  inc_hh_hist_raw <- 
    read_xls(
      path = 
        file.path(
          "raw_data", 
          "inc_hh_hist_raw.xls"
          ), 
      range = "A7:G58", 
      col_names = 
        c("year", 
          "number_thousands", 
          "lowest", 
          "second", 
          "third", 
          "fourth", 
          "lower_limit_top_5_pct")
      )
  
  # save the data
  save(
    inc_hh_hist_raw, 
    file = 
      file.path(
        "raw_data", 
        "inc_hh_hist_raw.rdata"
      )
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    inc_hh_hist_raw
    )
  invisible(gc())
}
   
   
   
```


### ruca

```{r ruca data download, eval=FALSE, include=FALSE}

# source 
# https://www.ers.usda.gov/webdocs/DataFiles/53241/ruca2010revised.xlsx?v=8632.5
# downloaded 2019-07-12
# last updated 2019-07-03 per 
# https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes.aspx

if(
  !file.exists(
    file.path(
      "raw_data", 
      "ruca_2010_raw.rdata"
      )
    )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # download files
  file_downloader(
    desired_filename_as_string = "ruca_2010_raw.xlsx", 
    web_address_as_string = 
      "https://www.ers.usda.gov/webdocs/DataFiles/53241/ruca2010revised.xlsx?v=8632.5", 
    folder_as_string = "raw_data"
    )
  
  # read the data from the excel file
  ruca_2010_raw <- 
    read_xlsx(
      file.path(
        "raw_data", 
        "ruca_2010_raw.xlsx"
        )
      )
  
  # save the excel file
  save(
    ruca_2010_raw, 
    file = 
      file.path(
        "raw_data", 
        "ruca_2010_raw.rdata"
      )
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    ruca_2010_raw
    )
  invisible(gc())
}

```


## air pollution

### pm2.5

```{r pm2.5 total frm, eval=FALSE, include=FALSE}

# source https://aqs.epa.gov/aqsweb/airdata/ (see vector below for exact)
# downloaded 2019 June 19
# last updated 2019 May 28 (all files)

if(
  !file.exists(
    file.path(
      "pre_travis_intermediate_data", 
      "raw_pm_25_frm_tibble.rdata"
      )
    )
  ) {
  
  # load file downloader function
  load(
    file = 
      file.path(
       "functions", 
       "file_downloader.rdata"
      )
    )
  
  # load the column name fixer function
  load(
    file = 
      file.path(
       "functions", 
       "column_name_fixer.rdata"
      )
    )

  # make list of files to download
  pm_25_frm_files <- 
    c("https://aqs.epa.gov/aqsweb/airdata/daily_88101_2005.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2006.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2007.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2008.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2009.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2010.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2011.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2012.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2013.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2014.zip",
      "https://aqs.epa.gov/aqsweb/airdata/daily_88101_2015.zip")
  
  
  # make a list of filenames for the downloaded data using the above filenames
  pm_25_frm_zip_filename_list <-  # make list of what the filenames will be
    map_chr(pm_25_frm_files,  # witness the `map` function
            str_extract, 
            "daily_.+\\.zip")   # witness the regular expression!
  
  # download the files
  map2(pm_25_frm_zip_filename_list, 
       pm_25_frm_files, 
       file_downloader, 
       folder_as_string = "raw_data")
  
  # unzip all the files and save them to disk
  map(
    file.path(
      "raw_data",
      pm_25_frm_zip_filename_list
      ),
    unzip,
    exdir = "pre_travis_intermediate_data"
    )
  
  # replace ".zip" with ".csv" in pm_25_frm_zip_filename_list
  pm_25_frm_csv_filename_list <-
    map(pm_25_frm_zip_filename_list,
        str_replace,
        "zip",
        "csv")
  
  # make list of tibble names by removing the ".zip" extension
  pm_25_frm_tibble_name_list <-
    map(pm_25_frm_zip_filename_list,
        str_replace,
        ".zip",
        "")
  
  # read .csv files into tibbles!
  if(
    !file.exists(
      file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
      )
    ) {  # checks if tibbles already exist
    pm_25_frm_tibble_list <-  # if not reads .csv files into tibbles
      map(
        file.path(
          "pre_travis_intermediate_data",
          pm_25_frm_csv_filename_list),
        read_csv
        )
    names(pm_25_frm_tibble_list) <-  # names the tibbles
      pm_25_frm_tibble_name_list
    save(
      pm_25_frm_tibble_list,  # saves the tibbles to disk to avoid doing this again
      file = file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
      )
    
  } else {
      load(
        file = file.path(
        "pre_travis_intermediate_data",
        "pm_25_frm_tibble_list.rdata"
        )
        )  # loads the tibbles if they don't already exist
  }
  
  # fix column names
  pm_25_frm_tibble_list <-
    map(pm_25_frm_tibble_list,
        column_name_fixer)
  
  # combine air data tibbles into one big tibble
  raw_pm_25_frm_tibble <-
    bind_rows(pm_25_frm_tibble_list)
  
  # save all pm2.5 total data to disk
  save(
      raw_pm_25_frm_tibble,  # saves the tibbles to disk to avoid doing this again
      file = file.path(
        "pre_travis_intermediate_data",
        "raw_pm_25_frm_tibble.rdata"
        )
      )
  
  # memory and disk clearance
  
  # delete individual .csv files from disk
  map(
    file.path(
      "pre_travis_intermediate_data",
      pm_25_frm_csv_filename_list
      ),
    file.remove
    )
  
  # remove loaded and created objects from environment
  rm(
    file_downloader,
    column_name_fixer,
    pm_25_frm_csv_filename_list, 
    pm_25_frm_tibble_name_list, 
    pm_25_frm_files,
    pm_25_frm_zip_filename_list,
    pm_25_frm_tibble_list, 
    raw_pm_25_frm_tibble
    )
  
  # clear memory
  invisible(gc())

}

```


### species

```{r speciation file download via speciator function, eval=FALSE, include=FALSE}

# source https://aqs.epa.gov/aqsweb/airdata/ (see vector below for exact)
# downloaded 2019 June 19
# last updated 2019 May 28 (all files)

# make speciator function to download speciation files
speciator <- 
  function(speciation_file_link) {
    
    # extract the file name from the link
    speciation_zip_filename <-
      str_extract(
        speciation_file_link,
        "daily_.+\\.zip"
        )
    
    # create rdata file name
    speciation_rdata_filename <- 
      str_replace(
        speciation_zip_filename,
        "zip",
        "rdata"
        )
    
     if(
      !file.exists(  # checks if tibble already exists
        file.path(
          "pre_travis_intermediate_data",
          speciation_rdata_filename
          )
        )
      ) {
       
      # load file downloader function
      load(
        file = 
          file.path(
           "functions", 
           "file_downloader.rdata"
          )
        )
       
       # load the column name fixer function
       load(
         file = 
           file.path(
             "functions", 
             "column_name_fixer.rdata"
             )
         )
       
       # download the file
       file_downloader(
         desired_filename_as_string = speciation_zip_filename,
         web_address_as_string = speciation_file_link,
         folder_as_string = "raw_data"
       )
    
       # unzip the file
       unzip(
         file.path(
           "raw_data",
           speciation_zip_filename
           ), 
         exdir = "pre_travis_intermediate_data"
         )
      
       # replace ".zip" with ".csv" in speciation_zip_filename
       speciation_csv_filename <- 
         str_replace(
           speciation_zip_filename, 
           "zip", 
           "csv")
      
       speciation_tibble_name <- 
         str_replace(
           speciation_zip_filename, 
           ".zip",
           ""
         )
      
       # read .csv file into a tibble
         speciation_tibble <- 
           read_csv(
             file.path(
               "pre_travis_intermediate_data",
               speciation_csv_filename
               )
             )
         
       # fix column names
       speciation_tibble <-
         column_name_fixer(
           speciation_tibble
         )
       
       # rename it to its filename
       assign(
         speciation_tibble_name, 
         speciation_tibble
       )
        
       # save the result to disk
       save(
         list = speciation_tibble_name,
         file = 
           file.path(
             "pre_travis_intermediate_data",
             speciation_rdata_filename
             )
       )
       
       # delete .csv file from disk
       map(
         file.path(
           "pre_travis_intermediate_data",
           speciation_csv_filename
           ),
         file.remove
         )
      
       # remove loaded and created objects from environment
       rm(
         file_downloader,
         column_name_fixer,
         speciation_tibble
         )
       invisible(gc())
     }
    
    # remove files
    rm(
      speciation_zip_filename, 
      speciation_rdata_filename
    )
    invisible(gc())
  }

# make list of speciation web addresses
speciation_files <-
  c("https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2005.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2006.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2007.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2008.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2009.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2010.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2011.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2012.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2013.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2014.zip",
    "https://aqs.epa.gov/aqsweb/airdata/daily_SPEC_2015.zip")

map(
  .x = speciation_files, 
  .f = speciator
)

rm(speciation_files)
invisible(gc())

```


```{r species big tibble, eval=FALSE, include=FALSE}

if(
  !file.exists(  # checks if tibble already exists
    file.path(
      "pre_travis_intermediate_data",
      "raw_speciation_tibble.rdata"
      )
    )
  ) {
  # make list of speciation file names except the file names of the first two
  speciation_filenames_except_2005_2006 <- 
    sprintf("daily_SPEC_%d.rdata", 
            seq(2007, 2015, 1))

  # load 2005 speciation file
  load(
    file.path(
      "pre_travis_intermediate_data", 
      "daily_SPEC_2005.rdata"
      )
    )
  
  # load 2006 speciation file
  load(
    file.path(
      "pre_travis_intermediate_data", 
      "daily_SPEC_2006.rdata"
      )
    )
  
  # create the first edition of the big speciation tibble by a row bind
  raw_speciation_tibble <- 
    bind_rows(
      daily_SPEC_2005, 
      daily_SPEC_2006
    )
  
  # remove the 2005 and 2006 tibbles from memory
  rm(
    daily_SPEC_2005, 
    daily_SPEC_2006)
  invisible(gc())
  
  # make a function to bind the rest of the speciation tibbles
  speciebinder <- 
    function(speciation_filename) {
      
      # make variable of tibble name
      speciation_tibble_name <- 
        str_replace(
          speciation_filename, 
          ".rdata",
          ""
          )
      
      # load next speciation file
      load(
        file.path(
          "pre_travis_intermediate_data", 
          speciation_filename
        )
      )
      
      # bind the speciation file to the big tibble
      raw_speciation_tibble <<- 
        bind_rows(  
          raw_speciation_tibble, 
          eval(as.symbol(speciation_tibble_name)))
      
      # remove the speciation file and clear the memory
      rm(speciation_filename, 
         list = speciation_tibble_name)
      invisible(gc())
      
    }
  
  # make the big speciation dataset
  speciebinder("daily_SPEC_2007.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2008.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2009.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2010.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2011.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2012.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2013.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2014.rdata")
  invisible(gc())
  speciebinder("daily_SPEC_2015.rdata")
  invisible(gc())
  
  # save the big speciation dataset
  save(
    raw_speciation_tibble, 
    file = 
      file.path(
        "pre_travis_intermediate_data", 
        "raw_speciation_tibble.rdata"
        )
  )
  
  # remove non-needed files from memory
  rm(speciation_files, 
     speciation_filenames_except_2005_2006,
     raw_speciation_tibble)
  invisible(gc())
  }

```


### air pollution in one big tibble

```{r join pm25 total and speciation data, eval=FALSE, include=FALSE}

if(
  !file.exists(  # checks if tibble already exists
    file.path(
      "pre_travis_intermediate_data",
      "raw_air_tibble.rdata"
      )
    )
  ) {

  # load the total pm2.5 tibble
  load(
    file.path(
      "pre_travis_intermediate_data",
      "raw_pm_25_frm_tibble.rdata"
    )
  )

  # load the pm2.5 speciation tibble
  load(
    file.path(
      "pre_travis_intermediate_data",
      "raw_speciation_tibble.rdata"
    )
  )

  # and in the darkness bind them
  raw_air_tibble <-
    bind_rows(
      raw_pm_25_frm_tibble,
      raw_speciation_tibble)

  # remove extraneous files
  rm(raw_pm_25_frm_tibble,
     raw_speciation_tibble)
  invisible(gc())

  # save the big tibble
  save(
    raw_air_tibble,
    file =
      file.path(
        "pre_travis_intermediate_data",
        "raw_air_tibble.rdata"
      ))
  
  # remove it from memory
  rm(raw_air_tibble)
  invisible(gc())

}

```


### clean and shrink the air pollution data

```{r clean big air pollution tibble, eval=FALSE, include=FALSE}

# LOOK AT ORGANIC AND ELEMENTAL CARBON AGAIN

# load the big air tibble into memory
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "raw_air_tibble.rdata"
    )
)

# make a dataset for cleaning
working_air_tibble <- 
  raw_air_tibble

# # remove the raw air tibble from memory
# rm(raw_air_tibble)
# invisible(gc())

# # get variable names
# names(working_air_tibble)

# # get rows with unique state codes
# states_and_codes <- 
#   working_air_tibble %>% 
#   select(state_name, state_code) %>% 
#   unique()

# list of non_us codes
non_us_state_codes <- 
  c(
    # "72",  # PR (now including)
    "78",  # Virgin Islands (alas, no RUCA/census data)
    "CC",  # Canada
    "80"   # Mexico
    )

# list of parameter names and codes
species_and_codes_from_data <- 
  working_air_tibble %>% 
  select(parameter_name, parameter_code) %>% 
  unique() %>% 
  arrange(parameter_code)

# get observation counts
observations_counts_by_code <- 
  working_air_tibble %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(parameter_code)

# # get organic carbon codes
# organic_carbon_codes <- 
#   species_and_codes_from_data %>% 
#   filter(str_detect(parameter_name, "OC")) %>% 
#   pull(parameter_code)

# get OC/EC fraction codes
fraction_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "EC[0-9]") | 
           str_detect(parameter_name, "OC[0-9]")) %>% 
  pull(parameter_code)

# # get unadjusted codes
# unadjusted_codes <- 
#   species_and_codes_from_data %>% 
#   filter(str_detect(parameter_name, "nadjusted")) %>% 
#   pull(parameter_code)
  
# get OP carbon codes
op_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "OP")) %>% 
  pull(parameter_code)

# get codes for volatile/non-volatile nitrate
volatile_codes <- 
  species_and_codes_from_data %>% 
  filter(str_detect(parameter_name, "olatile")) %>% 
  pull(parameter_code)

# other codes to drop
remaining_drop_codes <- 
  c(
    88308,  # carbonate carbon
    88312,  # total carbon (Total Carbon PM2.5 LC TOT)
    88313,  # black carbon (Black Carbon PM2.5 at 880 nm)
    88314 #,   # UV carbon (UV Carbon PM2.5 at 370 nm)
    # 88316  # optical EC (Optical EC PM2.5 LC TOT)
  )

# remove non_states to make tibble of all monitors with any data in US
us_raw_air_tibble <- 
  working_air_tibble %>% 
  filter(!(state_code %in% non_us_state_codes))  # removes non-US sites

# make monitor id and monitor id with poc
us_raw_air_tibble <- 
  us_raw_air_tibble %>% 
  mutate(
    monitor_id = 
      paste(
        state_code, 
        county_code, 
        site_num, 
        sep="-"), 
    poc = str_pad(poc, 2, pad = "0"),
    monitor_id_poc = str_c(monitor_id, "-", poc)
    )

# save the plain us tibble
save(
  us_raw_air_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_raw_air_tibble.rdata"
  )
)

# remove the working air tibble
rm(working_air_tibble)
invisible(gc())

# drop observations not of interest
us_filtered_air_tibble <- 
  us_raw_air_tibble %>%  
  filter(parameter_code > 88100) %>%   # removes non-parameter variables
  filter(!(parameter_code %in% fraction_codes)) %>%  # remove fractions
  filter(!(parameter_code %in% op_codes)) %>%  # remove pyrolized carbon
  filter(!(parameter_code %in% volatile_codes)) %>%  # remove weird nitrate
#  filter(!(parameter_code %in% unadjusted_codes)) %>%  # remove unadjusteds
  filter(!(parameter_code %in% remaining_drop_codes))  # remove remaining drops

# remove the us raw tibble
rm(us_raw_air_tibble)
invisible(gc())
  
# stuff from data two
species_and_codes_from_data_two <- 
  us_filtered_air_tibble %>% 
  select(parameter_name, parameter_code) %>% 
  unique() %>% 
  arrange(parameter_code)

# count observations by code
observations_counts_by_code <- 
  us_filtered_air_tibble %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(parameter_code)

# save the us filtered air tibble
save(
  us_filtered_air_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_filtered_air_tibble.rdata"
    )
  )

# remove the remaining files
rm(non_us_state_codes, 
   species_and_codes_from_data, 
   fraction_codes, 
   op_codes, 
   volatile_codes,
   raw_air_tibble, 
   us_filtered_air_tibble, 
   species_and_codes_from_data_two,
   observations_counts_by_code, 
   remaining_drop_codes
   )
invisible(gc())

```


```{r split air tibble into travis-y pieces, eval=FALSE, include=FALSE}

# load big filtered air tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "us_filtered_air_tibble.rdata"
    )
)


# air pollutants only tibble
air_pollutant_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    monitor_id, 
    poc, 
    monitor_id_poc,
    parameter_code,
    sample_duration,
    date_local, 
    event_type, 
    observation_count, 
    observation_percent, 
    arithmetic_mean, 
    `1st_max_value`,
    `1st_max_hour`, 
    method_code
  )

save(
  air_pollutant_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_pollutant_tibble.rdata"
    )
)

rm(air_pollutant_tibble)
invisible(gc())


# method_tibble
method_tibble <-
  us_filtered_air_tibble %>% 
  select(parameter_code, 
         method_code,
         method_name) %>% 
  distinct() %>% 
  arrange(parameter_code, 
          method_code)

save(
  method_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "method_tibble.rdata"
    )
)

rm(method_tibble)
invisible(gc())


# parameter names and codes tibble
parameter_names_codes_tibble <- 
  us_filtered_air_tibble %>% 
  select(parameter_code, 
         parameter_name) %>% 
  distinct() %>% 
  arrange(parameter_code)

save(
  parameter_names_codes_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "parameter_names_codes_tibble.rdata"
    )
)

rm(parameter_names_codes_tibble)
invisible(gc())
  

# units of measure tibble
units_of_measure_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    parameter_code, 
    units_of_measure
  ) %>% 
  distinct()

save(
  units_of_measure_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "units_of_measure_tibble.rdata"
    )
)

rm(units_of_measure_tibble)
invisible(gc())


# monitor tibble
air_monitor_tibble <- 
  us_filtered_air_tibble %>% 
  select(
    monitor_id,
    state_code,
    county_code, 
    site_num, 
    latitude, 
    longitude, 
    datum, 
    local_site_name, 
    address, 
    state_name, 
    county_name, 
    city_name, 
    cbsa_name) %>% 
  distinct()

save(
  air_monitor_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_monitor_tibble.rdata"
    )
)

rm(air_monitor_tibble)
invisible(gc())


# total pm25 88101 tibble with 88101-only variables (pollutant standard, aqi)
total_88101_pollutant_standard_aqi_tibble <- 
  us_filtered_air_tibble %>% 
  filter(parameter_code == 88101) %>% 
  select(
    monitor_id, 
    poc, 
    parameter_code, 
    pollutant_standard, 
    date_local, 
    sample_duration,
    date_local, 
    event_type, 
    observation_count, 
    observation_percent, 
    arithmetic_mean, 
    `1st_max_value`,
    `1st_max_hour`, 
    aqi, 
    method_code
  )

save(
  total_88101_pollutant_standard_aqi_tibble, 
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

rm(total_88101_pollutant_standard_aqi_tibble, 
   us_filtered_air_tibble)
invisible(gc())

```


## travis cleanup

```{r save travis files in cheating folder, eval=FALSE, include=FALSE}

# save needed files from the work above to memory_savers

# state abbreviation character vector
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "state_abbreviation_character_vector.rdata"
    )
)

save(
  state_abbreviation_character_vector, 
  file = 
    file.path(
      "travis_cache", 
      "state_abbreviation_character_vector.rdata"
    )
)

rm(state_abbreviation_character_vector)
invisible(gc())


# geography files
load(
  file = 
    file.path(
      "raw_data", 
      "tigris_shapefiles_no_data.rdata"
    )
)

save(
  tigris_shapefiles_no_data, 
  file = 
    file.path(
      "travis_cache", 
      "tigris_shapefiles_no_data.rdata"
    )
)

rm(tigris_shapefiles_no_data)
invisible(gc())


# neighbor file
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "neighbor_tibble_long.rdata"
    )
)

save(
  neighbor_tibble_long, 
  file = 
    file.path(
      "travis_cache", 
      "neighbor_tibble_long.rdata"
    )
  )

rm(neighbor_tibble_long)
invisible(gc())  


# sf1 census 2010 data
load(
  file = 
    file.path(
      "raw_data", 
      "sf1_totalcensus_raw_dataframe.rdata"
      )
  )

save(
  sf1_totalcensus_raw_dataframe,
  file = 
    file.path(
      "travis_cache", 
      "sf1_totalcensus_raw_dataframe.rdata"
      )
  )

rm(sf1_totalcensus_raw_dataframe)
invisible(gc())


# acs 2012 data
load(
  file = 
    file.path(
      "raw_data", 
      "acs_totalcensus_raw_dataframe.rdata"
    )
)

save(
  acs_totalcensus_raw_dataframe,
  file = 
    file.path(
      "travis_cache", 
      "acs_totalcensus_raw_dataframe.rdata"
      )
  )

rm(acs_totalcensus_raw_dataframe)
invisible(gc())


# historical income tibble
load(
  file =
    file.path(
      "raw_data", 
      "inc_hh_hist_raw.rdata"
    )
)

save(
  inc_hh_hist_raw,
  file =
    file.path(
      "travis_cache", 
      "inc_hh_hist_raw.rdata"
    )
)

rm(inc_hh_hist_raw)
invisible(gc())


# ruca files
load(
  file =
    file.path(
      "raw_data",
      "ruca_2010_raw.rdata"
    )
  )

save(
  ruca_2010_raw,
  file =
    file.path(
      "travis_cache",
      "ruca_2010_raw.rdata"
    )
  )

rm(ruca_2010_raw)
invisible(gc())


# air pollutant only tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_pollutant_tibble.rdata"
    )
)

save(
  air_pollutant_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "air_pollutant_tibble.rdata"
    )
)

rm(air_pollutant_tibble)
invisible(gc())


# air monitor only tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "air_monitor_tibble.rdata"
    )
)

save(
  air_monitor_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "air_monitor_tibble.rdata"
    )
)

rm(air_monitor_tibble)
invisible(gc())


# method_tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "method_tibble.rdata"
    )
)

save(
  method_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "method_tibble.rdata"
    )
)

rm(method_tibble)
invisible(gc())


# parameter names and codes tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "parameter_names_codes_tibble.rdata"
    )
)

save(
  parameter_names_codes_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "parameter_names_codes_tibble.rdata"
    )
)

rm(parameter_names_codes_tibble)
invisible(gc())
  

# units of measure tibble
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "units_of_measure_tibble.rdata"
    )
)

save(
  units_of_measure_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "units_of_measure_tibble.rdata"
    )
)

rm(units_of_measure_tibble)
invisible(gc())


# total pm25 88101 tibble with 88101-only variables (pollutant standard, aqi)
load(
  file = 
    file.path(
      "pre_travis_intermediate_data", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

save(
  total_88101_pollutant_standard_aqi_tibble, 
  file = 
    file.path(
      "travis_cache", 
      "total_88101_pollutant_standard_aqi_tibble.rdata"
    )
)

rm(total_88101_pollutant_standard_aqi_tibble)
invisible(gc())

```


# TRAVIS SHALL BEGIN HERE

## geography 

```{r ruca data format}

# load column name fixer function
load(
  file = 
    file.path(
     "functions", 
     "column_name_fixer.rdata"
    )
  )

# read the data from the excel file
load(
  file = 
    file.path(
      "travis_cache", 
      "ruca_2010_raw.rdata"
    )
)
  
# fix column names
ruca_simple <-
  ruca_2010_raw %>% 
  column_name_fixer %>%  # fix column names
  select(
    tract_id = 4, 
    ruca_prime = 5, 
    ruca_second = 6
  ) %>% 
  mutate(urban = 'noturban')


# set to urban or not urban based on RUCA codes
ruca_simple <- 
  within(ruca_simple, 
         urban[ruca_second == 1.0 | 
               ruca_second == 1.1 | 
               ruca_second == 2.0 | 
               ruca_second == 2.1 | 
               ruca_second == 3.0 | 
               ruca_second == 3.1 | 
               ruca_second == 4.1 | 
               ruca_second == 5.1 | 
               ruca_second == 6.1 | 
               ruca_second == 7.1 | 
               ruca_second == 8.1 | 
               ruca_second == 9.1 | 
               ruca_second == 10.1] <- 
           'urban')

ruca_urban_only <- 
  ruca_simple %>% 
  filter(
    urban == 'urban'
    )

# save useful files
save(
  ruca_simple, 
  file = 
    file.path(
      "intermediate_data", 
      "ruca_simple.rdata"
    )
)

save(
  ruca_urban_only, 
  file = 
    file.path(
      "intermediate_data", 
      "ruca_urban_only.rdata"
    )
)

# remove the shapefile from memory
rm(column_name_fixer, 
   ruca_2010_raw, 
   ruca_simple,
   ruca_urban_only)
invisible(gc())

```

## demographic file formatting

```{r census data format}

# load the column name fixer function
load(
  file = 
    file.path(
     "functions", 
     "column_name_fixer.rdata"
    )
  )

# load raw sf1 census dataframe
load(
  file = 
    file.path(
      "travis_cache", 
      "sf1_totalcensus_raw_dataframe.rdata"
    )
)
# make the dataframe into a tibble
sf1_totalcensus_tibble <- 
  as_tibble(sf1_totalcensus_raw_dataframe)

# fix column names 
sf1_totalcensus_tibble <- 
  column_name_fixer(
    sf1_totalcensus_tibble
)

# fix geoids
sf1_totalcensus_tibble <- 
  sf1_totalcensus_tibble %>% 
  mutate(
    geoid = 
      str_replace(
        geoid,
        "14000US", 
        ""
      )
  )

# save the tibble to disk
save(
  sf1_totalcensus_tibble, 
  file = 
    file.path(
      "intermediate_data",
      "sf1_totalcensus_tibble.rdata"
      )
  )
  
# remove loaded and created objects from environment
rm(
  column_name_fixer,
  sf1_totalcensus_raw_dataframe, 
  sf1_totalcensus_tibble
  )
invisible(gc())

```


```{r acs data format}

# load the column name fixer function
load(
  file = 
    file.path(
     "functions", 
     "column_name_fixer.rdata"
    )
  )

# load raw dataframe
load(
  file = 
    file.path(
      "travis_cache", 
      "acs_totalcensus_raw_dataframe.rdata"
    )
)

# make it into a tibble
acs_totalcensus_tibble <- 
  as_tibble(acs_totalcensus_raw_dataframe)

# fix variable names
acs_totalcensus_tibble <- 
  acs_totalcensus_tibble %>% 
  column_name_fixer()

# fix geoids
acs_totalcensus_tibble <- 
  acs_totalcensus_tibble %>% 
  mutate(
    geoid = 
      str_replace(
        geoid,
        "14000US", 
        ""
      )
  )

# save
save(
  acs_totalcensus_tibble, 
  file = 
    file.path(
      "intermediate_data", 
      "acs_totalcensus_tibble.rdata"
    )
)

rm(
  column_name_fixer, 
  acs_totalcensus_raw_dataframe, 
  acs_totalcensus_tibble)
invisible(gc())

```


```{r 2012 household income quantiles data format}

# load raw historical data
load(
  file = 
    file.path(
      "travis_cache", 
      "inc_hh_hist_raw.rdata"
    )
)

# pull the cutoffs for the 2012 household income quantiles for ice
inc_hh_2012_cutoffs <- 
  inc_hh_hist_raw %>% 
  filter(
    year == "2012"
  ) %>% 
  select(
    lowest, 
    second, 
    third, 
    fourth
  )

# save file to disk
save(
  inc_hh_2012_cutoffs, 
  file = 
    file.path(
      "intermediate_data", 
      "inc_hh_2012_cutoffs.rdata"
      )
)

# remove variables from working memory
rm(inc_hh_hist_raw, 
   inc_hh_2012_cutoffs)
invisible(gc())


```


## demographics

### making census 2010 geography match acs 2012 geography

```{r geography changes from 2010 to 2011}

# https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes/2011/geography-changes.html

# https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes/2012/geography-changes.html

load(
  file = 
    file.path(
      "intermediate_data", 
      "acs_totalcensus_tibble.rdata"
    )
)

load(
  file = 
    file.path(
      "intermediate_data", 
      "sf1_totalcensus_tibble.rdata"
      )
  )

# character vector of geoids from 2012 acs data
geoids_acs <- 
  acs_totalcensus_tibble %>% 
  pull(geoid)

# character vector of geoids from 2010 census sf1 data
geoids_sf1 <- 
  sf1_totalcensus_tibble %>% 
  pull(geoid)

# character vector of 2012 acs geoids not in 2010 census sf1
acs_geoids_not_in_sf1 <- 
  setdiff(geoids_acs, geoids_sf1)

# character vector of 2012 acs geoids not in 2010 census sf1
sf1_geoids_not_in_acs <- 
  setdiff(geoids_sf1, geoids_acs)

# manual acs geoids corresponding to and ordered by sf1_geoids_not_in_acs
acs_geoids_for_sf1_geoids_not_in_acs <- 
  c("04019002704",
    "04019002906",
    "04019004118",
    "04019004121",
    "04019004125",
    "04019005200",
    "04019005300",
    "06037137000",
    "36053030101",
    "36053030102",
    "36053030103",
    "36053030200",
    "36053030300",
    "36053030401",
    "36053030403",
    "36053030600",
    "36053030402",
    "36065024800",
    "36065024700",
    "36065024900",
    "36085009700")

# manual sf1 2010 geoids of census tracts with boundary changes in acs 2012
sf1_messed_up_tracts <- 
  c("06037930401", 
    "36065940200", 
    "36085008900", 
    "36065023000", 
    "06037800204")

# make concordance tibble
sf1_acs_concordance_tibble <- 
  tibble(sf1_geoid = sf1_geoids_not_in_acs, 
         acs_geoid = acs_geoids_for_sf1_geoids_not_in_acs)

# add acs equivalent-ish ids to sf1 table
sf1_tibble_with_acs_concordance <- 
  sf1_totalcensus_tibble %>%   # summon the census
  mutate(geoid_for_acs_join = geoid) %>%  # make new variable to use to join acs 
  mutate(geoid_for_acs_join =  # replace sf1 geoids not found in acs with 
           str_replace_all(    # equivalent acs geoids
             geoid_for_acs_join,  # this is the column to be modified
             setNames(  # this names the replacement values by their sf1 ids
               acs_geoids_for_sf1_geoids_not_in_acs,  # replacements'("objects")
               sf1_geoids_not_in_acs  # values to be replaced, or "names"
               )  # thus this identifies the sf1 geoids as "names" and replaces
             )    # them with their "object" values
         ) 

# check differences
acs_concordance_geoids_not_in_sf1 <- 
  setdiff(
    sf1_tibble_with_acs_concordance$geoid_for_acs_join, 
    sf1_tibble_with_acs_concordance$geoid
  )

sf1_geoids_not_in_acs_concordance_geoids <- 
  setdiff(
    sf1_tibble_with_acs_concordance$geoid,
    sf1_tibble_with_acs_concordance$geoid_for_acs_join
  )

# save files
save(sf1_acs_concordance_tibble, 
     file = 
       file.path(
         "intermediate_data", 
         "sf1_acs_concordance_tibble.rdata"
       )
     )
save(sf1_tibble_with_acs_concordance, 
     file = 
       file.path(
         "intermediate_data", 
         "sf1_tibble_with_acs_concordance.rdata"
       )
     )
    
save(sf1_messed_up_tracts, 
     file = 
       file.path(
         "intermediate_data", 
         "sf1_messed_up_tracts.rdata"
       )
     )

# remove the files
rm(
  acs_totalcensus_tibble, 
  sf1_totalcensus_tibble, 
  geoids_acs, 
  geoids_sf1, 
  acs_geoids_not_in_sf1, 
  sf1_geoids_not_in_acs, 
  acs_geoids_for_sf1_geoids_not_in_acs, 
  sf1_messed_up_tracts,
  sf1_acs_concordance_tibble, 
  sf1_tibble_with_acs_concordance, 
  acs_concordance_geoids_not_in_sf1, 
  sf1_geoids_not_in_acs_concordance_geoids
)
invisible(gc())
```


### segregation

```{r calculate segregation index}

# load census dataset
load(
  file = 
    file.path(
      "intermediate_data", 
      "sf1_totalcensus_tibble.rdata"
    )
)

# load census tract neighbor data
load(
  file = 
    file.path(
      "travis_cache", 
      "neighbor_tibble_long.rdata"
    )
)

# import segregation variables (Black, Black/other, total)
seg_vars <- 
  sf1_totalcensus_tibble %>% 
  select(  # import the variables
    geoid, 
    p0050001, 
    p0050004
  ) %>%  # make total nonblack population variable
  mutate(
    tot_pop_no_black = p0050001 - p0050004
  ) 

# merge by the NEIGHBOR TRACT GEOID
neighbors_with_census_tibble <- 
  neighbor_tibble_long %>% 
  left_join(
    seg_vars, 
    by = c("geoid_neighbor_tract" = "geoid")
  )

# get summaries by each PRIMARY TRACT for it and all its neighbors
racial_isolation_index_tibble <- 
  neighbors_with_census_tibble %>% 
  group_by(geoid_primary_tract) %>% 
  summarise(
    neighborly_tot_pop_p0050001 = sum(p0050001), 
    neighborly_black_p0050004 = sum(p0050004), 
    neighborly_tot_pop_no_black = sum(tot_pop_no_black)
  ) %>% 
  mutate(
    racial_isolation_index_k1 = 
      neighborly_black_p0050004/
      neighborly_tot_pop_p0050001) %>% 
  select(
    geoid_primary_tract, 
    racial_isolation_index_k1
  )

# save the final file
save(
  racial_isolation_index_tibble, 
  file = 
    file.path(
      "intermediate_data", 
      "racial_isolation_index_tibble.rdata"
    )
)

# remove extraneous files
rm(
  sf1_totalcensus_tibble, 
  neighbor_tibble_long, 
  seg_vars,
  neighbors_with_census_tibble, 
  racial_isolation_index_tibble
)

```


### merge sf1, acs, and segregation files

```{r merge sf1, acs, and segregation}

# load sf1 tibble
load(
  file = 
    file.path(
      "intermediate_data", 
      "sf1_tibble_with_acs_concordance.rdata"
    )
)

# load acs tibble
load(
  file = 
    file.path(
      "intermediate_data", 
      "acs_totalcensus_tibble.rdata"
    )
)

# load segregation
load(
  file = 
    file.path(
      "intermediate_data", 
      "racial_isolation_index_tibble.rdata"
    )
)

# delete duplicate variables from acs tibble
acs_tibble_for_joining <- 
  acs_totalcensus_tibble %>% 
  rename(pop_acs = population, 
         geoid_acs = geoid) %>% 
  select(
    -c(name, 
       geocomp, 
       sumlev, 
       state,
       stusab,
       lon, 
       lat)
  )

# join everything
demographic_tibble <- 
  sf1_tibble_with_acs_concordance %>% 
  left_join(
    acs_tibble_for_joining, 
    by = c("geoid_for_acs_join" = "geoid_acs")  # sf1 variable left, acs right
  ) %>% 
  left_join(
    racial_isolation_index_tibble, 
    by = c("geoid" = "geoid_primary_tract")  # sf1 variable left, acs right
  )

# save to file
save(
  demographic_tibble, 
  file = 
    file.path(
      "intermediate_data", 
      "demographic_tibble.rdata"
    )
)

# remove files
rm(
  sf1_tibble_with_acs_concordance, 
  acs_totalcensus_tibble, 
  racial_isolation_index_tibble, 
  acs_tibble_for_joining, 
  demographic_tibble
)
invisible(gc())

```


### add land area

```{r land area to demographic tibble}

# load tigris shapefiles
load(
  file = 
    file.path(
      "travis_cache", 
      "tigris_shapefiles_no_data.rdata"
    )
)

# pull the geoid, land area, water area, drop geography
area_tibble <- 
  tigris_shapefiles_no_data %>% 
  st_drop_geometry() %>%  # drop the geometry
  as_tibble() %>%  # make result into a tibble
  select(
    geoid = geoid10,  # select and rename geoid
    aland10,  # select land area
    awater10  # select water area
  )

# remove shapefiles
rm(tigris_shapefiles_no_data)
invisible(gc())

# load demographic tibble
load(
  file = 
    file.path(
      "intermediate_data", 
      "demographic_tibble.rdata"
    )
)

# join demographic tibble to area tibble
demographic_area_tibble <- 
  demographic_tibble %>% 
  left_join(
    area_tibble, 
    by = "geoid"
  )

# save demographic area tibble
save(
  demographic_area_tibble, 
  file = 
    file.path(
      "intermediate_data", 
      "demographic_area_tibble.rdata"
    )
)

# remove unneeded files
rm(area_tibble, demographic_tibble, demographic_area_tibble)
invisible(gc())

```


### construct derived variables

```{r deriving demographic variables}

# load demographic area tibble
load(
  file = 
    file.path(
      "intermediate_data", 
      "demographic_area_tibble.rdata"
    )
)

# # load 2012 household income tibble (don't actually need it)
# load(
#   file = 
#     file.path(
#       "intermediate_data", 
#       "inc_hh_2012_cutoffs.rdata"
#     )
# )

# make the constructed tibble
demographic_constructed <- 
  demographic_area_tibble

# remove the raw tibble to save memory
rm(demographic_area_tibble)
invisible(gc())

#### census

#### demographic variables

# total population sf1 p1 p0010001
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    pop_tot_p0010001 = population)

# land and water area and population density
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    land_area_m2 = aland10, 
    water_area_m2 = awater10, 
    land_area_km2 = aland10 / 1000000, 
    water_area_km2 = awater10 / 1000000, 
    pop_dense_km2 = population / land_area_km2)


# race and ethnicity sf1 p5
  # hispanic or latino origin by race
  # universe: total population
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    not_hisp_lat_pct_p0050002 = 
      p0050002 / p0050001,
    white_nhl_pct_p0050003 = 
      p0050003 / p0050001,
    black_nhl_pct_p0050004 = 
      p0050004 / p0050001, 
    aian_nhl_pct_p0050005 = 
      p0050005 / p0050001,
    asian_nhl_p0050006 = 
      p0050006 / p0050001,
    nhopi_nhl_p0050007 = 
      p0050007 / p0050001, 
    some_other_nhl_p0050008 =
      p0050008 / p0050001,
    two_more_nhl_p0050009 =
      p0050009 / p0050001,
    hisp_lat_pct_p0050010 = 
      p0050010 / p0050001,
    white_hl_p0050011 = 
      p0050011 / p0050001, 
    black_hl_p0050012 = 
      p0050012 / p0050001, 
    aian_hl_p0050013 = 
      p0050013 / p0050001, 
    asian_hl_p0050014 = 
      p0050014 / p0050001,
    nhopi_hl_p0050015 = 
      p0050015 / p0050001, 
    some_other_hl_p0050016 = 
      p0050016 / p0050001, 
    two_more_hl_p0050017 = 
      p0050017 / p0050001,
    aian_nhopi_nhl_pct_p0050005_07 = 
      (p0050005 + p0050007) / p0050001, 
    all_aian_nhopi_pct_p0050005_07_13_15 =
      (p0050005 + p0050007 + p0050013 + p0050015) / p0050001, 
    some_other_and_two_more_nhl_p0050008_009 = 
      (p0050008 + p0050009) / p0050001)


# female percentages sf1 p12
  # sex by age (SF1 P12)
  # universe: total population
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    female_pct_p0120026 = 
      p0120026 / p0120001
  )


# median age sf1 p13 p0130001
  # universe: total population
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    age_median_p0130001 = p0130001)


# age percentages sf1 p12
  # sex by age
  # universe: total population
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    age_0_19_pct = 
      (p0120003 +   # male under 5 years
       p0120004 +   # male 5 to 9 years
       p0120005 +   # male 10 to 14 years
       p0120006 +   # male 15 to 17 years
       p0120007 +   # male 18 and 19 years
       p0120027 +   # female under 5 years
       p0120028 +   # female 5 to 9 years
       p0120029 +   # female 10 to 14 years
       p0120030 +   # female 15 to 17 years
       p0120031) /  # female 18 and 19 years
      pop_tot_p0010001,  # divide by total population
    age_20_64_pct = 
      (p0120008 +   # male 20 years
       p0120009 +   # male 21 years
       p0120010 +   # male 22 to 24 years
       p0120011 +   # male 25 to 29 years
       p0120012 +   # male 30 to 24 years
       p0120013 +   # male 35 to 39 years
       p0120014 +   # male 40 to 44 years
       p0120015 +   # male 45 to 49 years
       p0120016 +   # male 50 to 54 years
       p0120017 +   # male 55 to 59 years
       p0120018 +   # male 60 and 61 years
       p0120019 +   # male 62 to 64 years
       p0120032 +   # female 20 years
       p0120033 +   # female 21 years
       p0120034 +   # female 22 to 24 years
       p0120035 +   # female 25 to 29 years
       p0120036 +   # female 30 to 34 years
       p0120037 +   # female 35 to 39 years
       p0120038 +   # female 40 to 44 years
       p0120039 +   # female 45 to 49 years
       p0120040 +   # female 50 to 54 years
       p0120041 +   # female 55 to 59 years
       p0120042 +   # female 60 and 61 years
       p0120043) /  # female 62 to 64 years
      pop_tot_p0010001,  # total population
    age_65_plus_pct = 
      (p0120020 +   # male 65 and 66 years
       p0120021 +   # male 67 to 69 years
       p0120022 +   # male 70 to 74 years
       p0120023 +   # male 75 to 79 years
       p0120024 +   # male 80 to 84 years
       p0120025 +   # male 85 years and over
       p0120044 +   # female 65 and 66 years
       p0120045 +   # female 67 to 69 years
       p0120046 +   # female 70 to 74 years
       p0120047 +   # female 75 to 79 years
       p0120048 +   # female 80 to 84 years
       p0120049) /  # female 85 years and over
    pop_tot_p0010001)  # total population


# housing tenure by household (renter and owner occupancy) sf1 h4
  # housing tenure (SF1 H4)
  # universe: occupied housing units
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    rent_occ_hh_pct_h0040004 = h0040004 / h0040001,
    own_occ_hh_pct = (h0040002 + h0040003) / h0040001
  )


# housing tenure by people (renter and owner occupancy) sf1 h11
  # population in occupied housing units by tenure
  # universe: population in occupied housing units
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    rent_occ_pop_pct = h0110004 / h0110001, 
    own_occ_pop_pct = (h0110002 + h0110003) / h0110001
  )



#### american community survey

# income household median b19013_001
  # median household income in the past 12 months (in 2012 inflation-adjusted dollars)
  # universe: households
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    inc_hh_med_b19013_001 = b19013_001, 
    inc_hh_med_b19013_001_margin = b19013_001_margin)


# unemployed percentage b25075
  # employment status for the population 16 years and over
  # universe: population 16 years and over
  # note: below uses % unemployed in civilian labor force only
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    unemp_pct_b25075_005_003 = b23025_005 / b23025_003, 
    unemp_pct_b25075_005_marg_num = b23025_005_margin, 
    unemp_pct_b25075_003_marg_denom = b23025_003_margin
    )


# median home value b25077
  # median value (dollars)
  # universe: owner-occupied housing units
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    home_val_med_b25077 = b25077_001, 
    home_val_med_b25077_marg = b25077_001_margin
  )

    
# at least high school education b15003
  # educational attainment for the population 25 years and over
  # universe: population 25 years and over
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    education_high_school_b15003_17_to_25 = 
      b15003_017 +
      b15003_018 +
      b15003_019 +
      b15003_020 +
      b15003_021 +
      b15003_022 +
      b15003_023 +
      b15003_024 +
      b15003_025
    )


# limited english percent of households b16002
  # household language by households in which no one 14 and over speaks english only or speaks a language other than english at home and speaks english "very well"
  # universe: households
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    ling_iso_pct_b16002_004_007_010_013 =
      (b16002_004 + b16002_007 + b16002_010 + b16002_013) / b16002_001
    )


# poverty percentages c17002
  # ratio of income to poverty level in the past 12 months
  # universe: population for whom poverty status is determined
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    pov_below_pct_c17002_002_to_003 = 
      (c17002_002 + 
       c17002_003) / 
       c17002_001,
    pov_below_125_pct_c17002_002_to_004 = 
      (c17002_002 + 
       c17002_003 + 
       c17002_004) / 
       c17002_001,
    pov_below_150_pct_c17002_002_to_005 = 
      (c17002_002 + 
       c17002_003 + 
       c17002_004 + 
       c17002_005) / 
       c17002_001,
    pov_below_185_pct_c17002_002_to_006 = 
      (c17002_002 + 
       c17002_003 + 
       c17002_004 + 
       c17002_005 + 
       c17002_006) / 
       c17002_001, 
    pov_below_200_pct_c17002_002_to_007 = 
      (c17002_002 + 
       c17002_003 + 
       c17002_004 + 
       c17002_005 + 
       c17002_006 + 
       c17002_007) / 
       c17002_001
  )


# median individual earnings b20002
  # median earnings in the past 12 months (in 2012 inflation-adjusted dollars) by sex for the population 16 years and over with earnings in the past 12 months
  # universe: population 16 years and over with earnings
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    earn_ind_med_b20002_001 = 
      b20002_001
  )


# health insurance percentage c27010
  # types of health insurance coverage by age
  # universe: civilian noninstitutionalized population
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    hlth_ins_none_pct_c27010_006_011_016_021 = 
      (c27010_006 +
       c27010_011 +
       c27010_016 +
       c27010_021) /
       c27010_001
    )


# # income concentration at the extremes (ice)
# # 2012 US Census historical income tables

# lowest  20599
# second  39764
# third   64582
# fourth  104096

# household income in the past 12 months (in 2012 inflation-adjusted dollars)
# universe: households
# B19001_001  total
# B19001_002  less than $10,000
# B19001_003  $10,000 to $14,999
# B19001_004  $15,000 to $19,999
# -- top of lowest quintile
# B19001_005  $20,000 to $24,999
# B19001_006  $25,000 to $29,999
# B19001_007  $30,000 to $34,999
# B19001_008  $35,000 to $39,999
# -- top of second quintile
# B19001_009  $40,000 to $44,999
# B19001_010  $45,000 to $49,999
# B19001_011  $50,000 to $59,999
# -- top of third quintile
# B19001_012  $60,000 to $74,999
# B19001_013  $75,000 to $99,999
# -- top of fourth quintile
# B19001_014  $100,000 to $124,999
# B19001_015  $125,000 to $149,999
# B19001_016  $150,000 to $199,999
# B19001_017  $200,000 or more

# acs_geo_cats$income_house_2017_bottom_quintile_24638 <- 
#   acs_geo_cats$B19001_002 +   # estimated as through $24999
#   acs_geo_cats$B19001_003 +
#   acs_geo_cats$B19001_004 +
#   acs_geo_cats$B19001_005
# acs_geo_cats$income_house_2017_top_quintile_126855 <- 
#   acs_geo_cats$B19001_015 +   # estimated as $125,000
#   acs_geo_cats$B19001_016 +
#   acs_geo_cats$B19001_017 
# acs_geo_cats$income_house_total_count_B19001_001 <-
#   acs_geo_cats$B19001_001
# 
# acs_geo_cats$ice_B19001 <- 
#   (acs_geo_cats$income_house_2017_top_quintile_126855 - 
#      acs_geo_cats$income_house_2017_bottom_quintile_24638) /
#   acs_geo_cats$income_house_total_count_B19001_001

# make quintile counts, total count, and ice variable
demographic_constructed <-
  demographic_constructed %>%
  mutate(
    ice_first_bottom_quintile_20599 =
      b19001_002 +
      b19001_003 +
      b19001_004, 
    ice_second_quintile_39764 = 
      b19001_005 +
      b19001_006 +
      b19001_007 +
      b19001_008, 
    ice_third_quintile_64582 =
      b19001_009 +
      b19001_010 +
      b19001_011, 
    ice_fourth_quintile_104096 = 
      b19001_012 +
      b19001_013, 
    ice_fifth_top_quintile = 
      b19001_014 +
      b19001_015 +
      b19001_016 +
      b19001_017,
    ice_total_count_b19001_001 = 
      b19001_001, 
    ice_b19001 = 
      (ice_fifth_top_quintile - 
         ice_first_bottom_quintile_20599) /
      ice_total_count_b19001_001
  )

# save constructed variables
save(
  demographic_constructed, 
  file = 
    file.path(
      "intermediate_data", 
      "demographic_constructed.rdata"
    )
)

# remove final file
rm(demographic_constructed)
invisible(gc())


```




## air data

### carbon disasters

```{r investigating carbon}

# load air data file
load(
  file = 
    file.path(
      "travis_cache", 
      "air_pollutant_tibble.rdata"
    )
)

# load pollutant names
load(
  file = 
    file.path(
      "travis_cache", 
      "parameter_names_codes_tibble.rdata"
))

# make tibble with pollutant names
names_pollutant_tibble <- 
  air_pollutant_tibble %>% 
  left_join(
    parameter_names_codes_tibble, 
    by = "parameter_code"
  )

# what are the pollutant names
pollutant_names <- 
  names_pollutant_tibble %>% 
  select(parameter_name, parameter_code) %>% 
  distinct()

# make a tibble for just the carbon
carbon_tibble <- 
  names_pollutant_tibble %>% 
  filter(str_detect(parameter_name, "EC") | 
           str_detect(parameter_name, "OC") | 
           str_detect(parameter_name, "carbon") |
            str_detect(parameter_name, "CARBON") |
           str_detect(parameter_name, "TC")
         )

# make a tibble for just elemental carbon
ec_tibble <- 
  carbon_tibble %>% 
  filter(
    str_detect(parameter_name, "EC")
  )

# make tibble of ec monitor counts by code
ec_parameter_codes_monitor_counts <- 
  ec_tibble %>% 
  select(monitor_id, poc, parameter_name, parameter_code) %>% 
  distinct() %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(desc(n))

# make organic carbon tibble
oc_tibble <- 
  carbon_tibble %>% 
  filter(
    str_detect(parameter_name, "OC")
  )

# make tibble of oc monitor counts by code
oc_parameter_codes_monitor_counts <- 
  oc_tibble %>% 
  select(monitor_id, poc, parameter_name, parameter_code) %>% 
  distinct() %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(desc(n))

# summarize each by date
carbon_by_date <- 
  carbon_tibble %>% 
  group_by(parameter_name, parameter_code) %>% 
  summarize(first_date = min(date_local), 
            last_date = max(date_local))

# summarize each by date and monitor
carbon_by_date_by_monitor <- 
  carbon_tibble %>% 
  group_by(monitor_id, parameter_name, parameter_code) %>% 
  summarize(first_date = min(date_local), 
            last_date = max(date_local)) %>% 
  arrange(monitor_id)

# count carbon parameter codes by monitor 
parameter_codes_monitor_counts <- 
  carbon_tibble %>% 
  select(monitor_id, poc, parameter_name, parameter_code) %>% 
  distinct() %>% 
  group_by(parameter_name, parameter_code) %>% 
  tally() %>% 
  arrange(desc(n))

# count overall number of monitors with carbon data
carbon_monitor_count <- 
  carbon_tibble %>% 
  select(monitor_id) %>% 
  distinct() %>% 
  tally()

# monitors with EC gold standard throughout the period
carbon_ec_monitor_id <- 
  carbon_tibble %>% 
  select(monitor_id, poc, parameter_name, parameter_code, date_local) %>% 
  filter(parameter_code == 88321) %>% 
  group_by(monitor_id, poc) %>% 
  summarize(min_date = min(date_local), 
            max_date = max(date_local)) %>% 
  arrange(min_date, max_date)

# make table of monitors
carbon_monitor_table <- 
  carbon_tibble %>% 
  select(monitor_id, poc, parameter_name, parameter_code, date_local) %>% 
  group_by(monitor_id, poc, parameter_code) %>% 
  summarise(first_date = min(date_local), 
            last_date = max(date_local), 
            observations = n()) %>% 
  arrange(monitor_id, poc, parameter_code)

# remove files from memory
rm(
  air_pollutant_tibble, 
  parameter_names_codes_tibble, 
  names_pollutant_tibble, 
  pollutant_names, 
  carbon_tibble, 
  ec_tibble, 
  ec_parameter_codes_monitor_counts, 
  oc_tibble, 
  oc_parameter_codes_monitor_counts, 
  carbon_by_date, 
  carbon_by_date_by_monitor, 
  parameter_codes_monitor_counts, 
  carbon_monitor_count, 
  carbon_ec_monitor_id, 
  carbon_monitor_table
)
invisible(gc())

```


```{r carbon conversions}

### load files

# load air data file
load(
  file = 
    file.path(
      "travis_cache", 
      "air_pollutant_tibble.rdata"
    )
)

# make list of data frames for conversion
converted_tibbles <- 
  list()

### elemental carbon

# convert 88307	EC CSN PM2.5 LC TOT
# 88321 EC PM2.5 LC TOR
converted_tibbles[["ec_88307_to_88321_conversion_tibble"]] <- 
  air_pollutant_tibble %>% 
  filter(parameter_code == 88307) %>% 
  mutate(parameter_code = 99307, 
         arithmetic_mean = arithmetic_mean / 0.71 - 0.08/0.71)

# convert 88380 EC CSN_Rev Unadjusted PM2.5 LC TOR to 
# 88321 EC PM2.5 LC TOR
converted_tibbles[["ec_88380_to_88321_conversion_tibble"]] <- 
  air_pollutant_tibble %>% 
  filter(parameter_code == 88380) %>% 
  mutate(parameter_code = 99380, 
         arithmetic_mean = arithmetic_mean / 0.98 - 0.02/0.98)


### organic carbon

# convert 88305	OC CSN Unadjusted PM2.5 LC TOT
# 88320 OC PM2.5 LC TOR
converted_tibbles[["ec_88305_to_88320_conversion_tibble"]] <- 
  air_pollutant_tibble %>% 
  filter(parameter_code == 88305) %>% 
  mutate(parameter_code = 99305, 
         arithmetic_mean = arithmetic_mean / 1.41 - 1.21/1.41)

# convert 88370 OC CSN_Rev Unadjusted PM2.5 LC TOR to 
# 88320 OC PM2.5 LC TOR
converted_tibbles[["ec_88370_to_88320_conversion_tibble"]] <- 
  air_pollutant_tibble %>% 
  filter(parameter_code == 88370) %>% 
  mutate(parameter_code = 99370, 
         arithmetic_mean = arithmetic_mean / 1.04 - 0.20/1.04)

# combine all of the above
air_pollutant_conversion_tibble <- 
  air_pollutant_tibble %>% 
  bind_rows(converted_tibbles) 
  

# save the air pollution conversion tibble
save(
  air_pollutant_conversion_tibble, 
  file = 
    file.path(
      "intermediate_data", 
      "air_pollutant_conversion_tibble.rdata"
    )
)

# remove tibbles
rm(air_pollutant_tibble,
   air_pollutant_conversion_tibble, 
   converted_tibbles)
invisible(gc())
  
```


```{r elemental carbon variable selection}

# load air pollution with conversions
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_pollutant_conversion_tibble.rdata"
    )
)

# load parameter names and codes
load(
  file = 
    file.path(
      "travis_cache", 
      "parameter_names_codes_tibble.rdata"
    )
)


# numeric vectors of converted and unconverted codes for ec
ec_converted_codes <- 
  c(99307, 99380)
ec_unconverted_codes <- 
  c(88307, 88380)

# make tibble of ec names and codes
ec_names_and_codes <- 
  parameter_names_codes_tibble %>% 
  filter(str_detect(parameter_name, "EC"))

# make numeric vector of all ec codes
ec_codes <- 
  parameter_names_codes_tibble %>% 
  filter(str_detect(parameter_name, "EC")) %>% 
  pull(parameter_code) %>% 
  combine(ec_converted_codes)

# filter tibble for only elemental carbon data considered for inclusion
ec_conversion_tibble <- 
  air_pollutant_conversion_tibble %>% 
  filter(
    parameter_code %in%
      ec_codes
  ) %>% 
  filter(
    !(parameter_code %in% 
        ec_unconverted_codes)  # take out raw pre-conversion rows
  )

# remove the air pollutant conversion tibble
rm(air_pollutant_conversion_tibble)
invisible(gc())

# ec conversion tibble
# summarize each by date and monitor
ec_monitor_tibble <- 
  ec_conversion_tibble %>% 
  select(monitor_id, poc, parameter_code, date_local) %>% 
  group_by(monitor_id, poc, parameter_code) %>% 
  summarise(first_date = min(date_local), 
            last_date = max(date_local), 
            observations = n()) %>% 
  arrange(monitor_id, poc, parameter_code)

# counts of poc codes by monitor_id
ec_pocs_by_site <- 
  ec_monitor_tibble %>% 
  select(monitor_id, poc) %>% 
  distinct() %>% 
  group_by(monitor_id) %>% 
  mutate(poc = as.character(poc)) %>% 
  summarize(poc_count = n()) %>% 
  arrange(poc_count)

# monitor id character vector
ec_monitor_ids <- 
  ec_conversion_tibble %>% 
  select(monitor_id) %>% 
  distinct() %>% 
  pull()

# make tibble of parameter codes and priority rankings
ec_priority_tibble <- 
  tibble(
    parameter_code = 
      c(88321, 99380, 99307, 88381, 88357, 88316),
    parameter_priority = 
      c(1, 2, 3, 4, 5, 6)
  )

# tibble including only the best data point for each day
ec_select_tibble <- 
  ec_conversion_tibble %>% 
  left_join(ec_priority_tibble, 
            by = "parameter_code") %>% 
  arrange(monitor_id_poc, date_local) %>% 
  group_by(monitor_id_poc, date_local) %>% 
  arrange(parameter_priority, .by_group = TRUE) %>% 
  slice(1)

# checking the select and full ec tibbles for all the date-monitor-id-poc combinations
ec_conversion_date_check <- 
  ec_conversion_tibble %>% 
  select(
    monitor_id_poc, 
    date_local
  ) %>% 
  distinct() %>% 
  tally()

ec_selected_date_check <- 
  ec_select_tibble %>% 
  ungroup() %>% 
  select(
    monitor_id_poc, 
    date_local
  ) %>% 
# distinct() %>% 
  tally()

# save the select tibble to disk
save(ec_select_tibble, 
     file = 
       file.path(
         "intermediate_data", 
         "ec_select_tibble.rdata"
       )
     )
  

rm(parameter_names_codes_tibble, 
   ec_converted_codes, 
   ec_unconverted_codes, 
   ec_names_and_codes, 
   ec_codes,
   ec_conversion_tibble, 
   ec_monitor_tibble, 
   ec_pocs_by_site, 
   ec_monitor_ids, 
   ec_priority_tibble,
   ec_select_tibble, 
   ec_conversion_date_check, 
   ec_selected_date_check)
invisible(gc())

  
```


```{r organic carbon variable selection}

# load air pollution with conversions
load(
  file = 
    file.path(
      "intermediate_data", 
      "air_pollutant_conversion_tibble.rdata"
    )
)

# load parameter names and codes
load(
  file = 
    file.path(
      "travis_cache", 
      "parameter_names_codes_tibble.rdata"
    )
)

# numeric vectors of converted and unconverted codes for ec and oc
oc_converted_codes <- 
  c(99305, 99370)
oc_unconverted_codes <- 
  c(88305, 88370)

# make tibble of oc names and codes
oc_names_and_codes <- 
  parameter_names_codes_tibble %>% 
  filter(str_detect(parameter_name, "OC"))

# make numeric vector of all oc codes
oc_codes <- 
  parameter_names_codes_tibble %>% 
  filter(str_detect(parameter_name, "OC")) %>% 
  pull(parameter_code) %>% 
  combine(oc_converted_codes)

# filter tibble for only organic carbon data considered for inclusion
oc_conversion_tibble <- 
  air_pollutant_conversion_tibble %>% 
  filter(
    parameter_code %in%
      oc_codes
  ) %>% 
  filter(
    !(parameter_code %in% 
        oc_unconverted_codes)  # take out raw pre-conversion rows
  )

# remove the air pollutant conversion tibble
rm(air_pollutant_conversion_tibble)
invisible(gc())

# oc conversion tibble
# summarize each by date and monitor
oc_monitor_tibble <- 
  oc_conversion_tibble %>% 
  select(monitor_id, poc, parameter_code, date_local) %>% 
  group_by(monitor_id, poc, parameter_code) %>% 
  summarise(first_date = min(date_local), 
            last_date = max(date_local), 
            observations = n()) %>% 
  arrange(monitor_id, poc, parameter_code)

# counts of poc codes by monitor_id
oc_pocs_by_site <- 
  oc_monitor_tibble %>% 
  select(monitor_id, poc) %>% 
  distinct() %>% 
  group_by(monitor_id) %>% 
  mutate(poc = as.character(poc)) %>% 
  summarize(poc_count = n()) %>% 
  arrange(poc_count)

# monitor id character vector
oc_monitor_ids <- 
  oc_conversion_tibble %>% 
  select(monitor_id) %>% 
  distinct() %>% 
  pull()

# make tibble of parameter codes and priority rankings
oc_priority_tibble <- 
  tibble(
    parameter_code = 
      c(88320, 99370, 99305, 88382, 88355),
    parameter_priority = 
      c(1, 2, 3, 4, 5)
  )

# tibble including only the best data point for each day
oc_select_tibble <- 
  oc_conversion_tibble %>% 
  left_join(oc_priority_tibble,  # add the priority codes
            by = "parameter_code") %>% 
  arrange(monitor_id_poc, date_local) %>% 
  group_by(monitor_id_poc, date_local) %>% 
  arrange(parameter_priority, .by_group = TRUE) %>%  # sort in priority order
  slice(1)  # take highest priority row for each monitor and date

# checking the select and full oc tibbles for all the date-monitor-id-poc combinations
oc_conversion_date_check <- 
  oc_conversion_tibble %>% 
  select(
    monitor_id_poc, 
    date_local
  ) %>% 
  distinct() %>% 
  tally()

oc_selected_date_check <- 
  oc_select_tibble %>% 
  ungroup() %>% 
  select(
    monitor_id_poc, 
    date_local
  ) %>% 
# distinct() %>% 
  tally()

save(oc_select_tibble, 
     file = 
       file.path(
         "intermediate_data", 
         "oc_select_tibble.rdata"
       )
     )

# remove files used
rm(parameter_names_codes_tibble, 
   oc_converted_codes, 
   oc_unconverted_codes, 
   oc_names_and_codes, 
   oc_codes,
   oc_conversion_tibble, 
   oc_monitor_tibble, 
   oc_pocs_by_site, 
   oc_monitor_ids, 
   oc_priority_tibble,
   oc_select_tibble, 
   oc_conversion_date_check, 
   oc_selected_date_check)
invisible(gc())

```


```{r make graphs by monitor, eval=FALSE, include=FALSE}

# thingie <- 
#   ggplot(
#     data = ec_conversion_tibble[ec_conversion_tibble$monitor_id_poc == "01-073-0023-05",],
#     aes(x = date_local, y = arithmetic_mean, color = factor(parameter_code))
#   ) + 
#   geom_point(aes(shape = factor(parameter_code)), size = 1, alpha = 0.5) +
#   scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "5 days", minor_breaks = "1 day") +
#   theme(axis.text.x = element_text(angle = 90, size = 3))
# 
# ggsave(filename = file.path("intermediate_data", "plots", "thingie.jpg"), plot = thingie, device = NULL, path = NULL,
#   scale = 1, width = 180, height = 5, units = c("cm"),
#   dpi = 300, limitsize = FALSE)
# 
# thingie

```


## monitor assignments

### assign monitors

```{r count and type monitor locations}

# load air pollutant tibble for monitor counts
load(
  file = 
    file.path(
      "travis_cache",
      "air_pollutant_tibble.rdata"
      )
  )

# isolate columns needed for monitor counting
monitor_id_and_code <- 
  air_pollutant_tibble %>% 
  dplyr::select(monitor_id,
                parameter_code) %>% 
  distinct()

# remove the air pollutant tibble to save memory
rm(air_pollutant_tibble)
invisible(gc())

# load the air monitor tibble
load(
  file = 
    file.path(
      "travis_cache",
      "air_monitor_tibble.rdata"
      )
  )

# isolate columns needed for monitor counting
monitor_locations <- 
  air_monitor_tibble %>% 
  dplyr::select(monitor_id, 
                latitude, 
                longitude, 
                datum)

# join parameter code information to monitor information
monitor_locations_and_parameter_codes <- 
  monitor_id_and_code %>% 
  left_join(monitor_locations, 
            by = "monitor_id")

# save the above
save(
  monitor_locations_and_parameter_codes, 
  file = 
    file.path(
      "intermediate_data",
      "monitor_locations_and_parameter_codes.rdata"
      )
  )

# vector for datums
datum_order <- 
  tibble(
    datum = 
      c(
        "NAD83", 
        "WGS84"
        )
    )

# order the monitor locations in order to convert the fewest possible
monitor_locations_and_parameter_codes_ordered <- 
  full_join(
    datum_order, 
    monitor_locations_and_parameter_codes, 
    by = "datum"
    )

# isolate unique monitor-parameter combinations
monitor_locations_and_parameter_codes_marked <-
  monitor_locations_and_parameter_codes_ordered %>% 
   distinct(
     monitor_id,
     parameter_code, 
     .keep_all = TRUE
     )

# classify monitors as for total pm25, species, or both
monitor_locations_and_parameter_codes_marked <- 
  monitor_locations_and_parameter_codes_marked %>%
  mutate(
    monitor_pm25 = 
      ifelse(
        (parameter_code == "88101"),
        "yes",
        "no"
        )
    ) %>% 
  mutate(
    monitor_species = 
      ifelse(
        (parameter_code != "88101"),
        "yes",
        "no"
        )
    ) %>%
  distinct(
    monitor_id,
    monitor_pm25,
    monitor_species, 
    .keep_all = TRUE
    )

# save the above
save(
  monitor_locations_and_parameter_codes_marked, 
  file = 
    file.path(
      "intermediate_data", 
      "monitor_locations_and_parameter_codes_marked.rdata"
      )
  )

# classify monitors as total pm2.5, species, or both
monitor_locations_and_parameter_codes_counts <- 
  monitor_locations_and_parameter_codes_marked %>% 
  add_count(monitor_id) %>% 
  mutate(monitor_type = 
           ifelse(
             n == 2, 
             "both", 
             ifelse(
               monitor_species == "yes",
               "species",
               ifelse(
                 monitor_pm25 == "yes", 
                 "pm25",
                 "what"
               )
             )
           )
         )

# only keep the two columns for monitor classification
sparse_monitors <- 
  monitor_locations_and_parameter_codes_counts %>% 
  distinct(monitor_id, 
           monitor_type,
           .keep_all = TRUE)

# save the monitor types and locations
save(
  sparse_monitors,
  file = 
    file.path(
      "intermediate_data", 
      "sparse_monitors.rdata"
      )
  )

# isolate NAD83 monitors
sparse_monitors_nad83 <- 
  sparse_monitors %>% 
  filter(datum == "NAD83")

# save NAD83 monitors
save(
  sparse_monitors_nad83,
  file = 
    file.path(
      "intermediate_data",
      "sparse_monitors_nad83.rdata"
      )
  )
  
# isolate WGS84 monitors
sparse_monitors_wgs84 <- 
  sparse_monitors %>% 
  filter(datum == "WGS84")

# save WGS84 monitors
save(
  sparse_monitors_wgs84,
  file = 
    file.path(
      "intermediate_data",
      "sparse_monitors_wgs84.rdata"
      )
  )

# remove the created files
rm(air_monitor_tibble,
   monitor_id_and_code, 
   monitor_locations,
   monitor_locations_and_parameter_codes, 
   datum_order,
   monitor_locations_and_parameter_codes_ordered, 
   monitor_locations_and_parameter_codes_marked, 
   monitor_locations_and_parameter_codes_counts,
   sparse_monitors,
   sparse_monitors_nad83, 
   sparse_monitors_wgs84
)

```


```{r count monitors by state}

load(
  file = 
    file.path(
      "intermediate_data", 
      "sparse_monitors.rdata"
      )
  )

load(
  file = 
    file.path(
      "travis_cache", 
      "air_monitor_tibble.rdata"
      )
  )

monitor_and_state <- 
  air_monitor_tibble %>% 
  select(
    monitor_id, 
    state_name, 
    state_code
  ) %>% 
  distinct()

# count monitors by type and state
state_monitor_counts <- 
  left_join(
    sparse_monitors,
    monitor_and_state, 
    by = "monitor_id"
  ) %>% 
  group_by(
    state_name, 
    monitor_type
  ) %>% 
  tally()

save(
  state_monitor_counts, 
  file = 
    file.path(
      "intermediate_data",
      "state_monitor_counts.rdata"
      )
)

rm(
  air_monitor_tibble,
  sparse_monitors,
  monitor_and_state, 
  state_monitor_counts
)
invisible(gc())
```


```{r make spatial files from monitor locations}

# load needed files
load(
  file = 
    file.path(
      "intermediate_data",
      "sparse_monitors_nad83.rdata"
    )
  )

load(
  file = 
    file.path(
      "intermediate_data",
      "sparse_monitors_wgs84.rdata"
    )
  )

load(
  file = 
    file.path(
      "travis_cache", 
      "tigris_shapefiles_no_data.rdata"
    )
)

# make monitor locations into spatial point files
sparse_monitors_nad83_sf <- 
  st_as_sf(
    sparse_monitors_nad83, 
    coords = 
      c("longitude",
        "latitude"),
    crs = st_crs(tigris_shapefiles_no_data) 
  ) 

sparse_monitors_wgs84_sf <- 
  st_as_sf(
    sparse_monitors_wgs84, 
    coords = 
      c("longitude",
        "latitude"),
    crs = 4326
  )

# transformations to match census data
sparse_monitors_wgs84_to_nad_83_sf <- 
  st_transform(
    sparse_monitors_wgs84_sf, 
    crs = st_crs(tigris_shapefiles_no_data))

monitors_sf <- 
  rbind(
    sparse_monitors_nad83_sf, 
    sparse_monitors_wgs84_to_nad_83_sf
  )

save(
  monitors_sf, 
  file = 
    file.path(
      "intermediate_data",
      "monitors_sf.rdata"
      )
  )

rm(
  sparse_monitors_nad83,
  sparse_monitors_wgs84,
  sparse_monitors_wgs84_to_nad_83_sf,
  sparse_monitors_nad83_sf,
  sparse_monitors_wgs84_sf,
  monitors_sf,
  tigris_shapefiles_no_data
  )
invisible(gc())
```


```{r assign tracts to monitors}

# load needed files
load(
  file = 
    file.path(
      "travis_cache", 
      "tigris_shapefiles_no_data.rdata"
    )
  )

# load monitor file
load(
  file = 
    file.path(
      "intermediate_data", 
      "monitors_sf.rdata"
    )
  )

# project the tracts to the NAD 83 Conus Albers Equal Area projection, a flat one, to make distances comparable
projected_tracts <- 
  tigris_shapefiles_no_data %>% 
  st_transform(crs = 5070) %>% 
  select(  # simplify to only needed variables
    geoid10, 
    aland10, 
    awater10, 
    intptlat10, 
    intptlon10
    ) %>% 
  mutate(  # calculate the total area of each census tract after projection (m2)
    proj_tract_area = 
      st_area(geometry)
  )

# set attributes for variables in projected tracts
st_agr(projected_tracts) <-  
  c(geoid10 = "identity", 
    aland10 = "aggregate", 
    awater10 = "aggregate", 
    intptlat10 = "aggregate", 
    intptlon10 = "aggregate", 
    proj_tract_area = "aggregate")
  
# project the monitors to the same
projected_monitors <-
  monitors_sf %>% 
  st_transform(crs = 5070) %>% 
  select(
    monitor_id, 
    monitor_type
  ) 

# join tracts to monitors
monitors_to_tracts <- 
  st_join(
    projected_monitors, 
    projected_tracts
  )

# save files
save(
  projected_tracts, 
  file = 
    file.path(
      "intermediate_data", 
      "projected_tracts.rdata"
    )
)

save(
  projected_monitors, 
  file = 
    file.path(
      "intermediate_data", 
      "projected_monitors.rdata"
    )
)

save(
  monitors_to_tracts, 
  file = 
    file.path(
      "intermediate_data", 
      "monitors_to_tracts.rdata"
    )
)

# remove files
rm(
  projected_monitors, 
  projected_tracts,
  monitors_sf, 
  tigris_shapefiles_no_data,
  monitors_to_tracts
)

```


```{r buffers}

# load projected monitor sf
load(
  file = 
    file.path(
      "intermediate_data", 
      "projected_monitors.rdata"
    )
  )

# make list in which to store the projected monitor sf
list_of_proj_mon_sf <- 
  list(projected_monitors)

# list of buffer distances desired in meters
distances <- 
  c(1000, 
    2000, 
    2500, 
    3000, 
    4000, 
    5000, 
    7500, 
    10000)

# make list of buffer names
buffer_names <- 
  str_c("buffer_", 
        as.character(distances), 
        "_m",
        sep = "")

# make the buffers
buffers <- 
  map2(.f = st_buffer, 
      .y = distances, 
      .x = list_of_proj_mon_sf) %>% 
  set_names(buffer_names)  # name the list elements

# set relation_to_geometry ("agr") attribute to "constant"
buffers <-
  map(
    .f = function(x) {
      st_agr(x) = 
        c(monitor_id = "identity", 
          monitor_type = "constant")
      return(x)
      },
    .x = buffers
  )

# save the buffers
save(
  buffers, 
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# remove unneeded files
rm(
  projected_monitors,
  list_of_proj_mon_sf, 
  distances, 
  buffer_names, 
  buffers
)
invisible(gc())

```


```{r unionize buffers}

# load buffers
load(
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# unionize buffers
unionized_buffers <- 
  map(
    .f = st_union,
    .x = buffers
  )

# save result
save(
  unionized_buffers,
  file = 
    file.path(
      "intermediate_data", 
      "unionized_buffers.rdata"
    )
)

# delete unneeded data
rm(buffers, 
   unionized_buffers)
invisible(gc())

```


```{r getting all buffer intersections for one tract}

# load projected tracts data
load(
  file = 
    file.path(
      "intermediate_data", 
      "projected_tracts.rdata"
    )
)

# load buffers
load(
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# extract one buffer layer and simplify it
one_buffer_size <- 
  buffers$buffer_10000_m %>% 
  select(monitor_id)

# extract one tract and simplify it
one_tract <- 
  projected_tracts %>% 
  filter(geoid10 == "02013000100") %>% 
  select(geoid10)

# clip buffers to only the one tract
buffers_in_tract <- 
  one_buffer_size %>% 
  st_intersection(one_tract, .) %>%  # this clips buffers (buffer sf is .)
  rowid_to_column("polygon_id")  # add row numbers to id polygons in next step

# make concordance vector to replace "origin" values (rows) with monitor ids
concordance_vector <- 
  setNames(
    object = buffers_in_tract$monitor_id,  # object to named
    nm = buffers_in_tract$polygon_id  # names (polygon #s)
  )

# find buffer intersections and areas by monitor
buffer_intersections <- 
  buffers_in_tract %>% 
  st_intersection() %>% 
  mutate(area = st_area(geometry),  # add areas
         monitor_id_overlaps =  # add column of monitor ids
           map(
             .x = origins,  # go through the origin column (with polygon #s)
             .f = function(a) concordance_vector[a]  # replace with monitor ids
             )
         ) %>% 
  select(-c(polygon_id, monitor_id))  # remove origin polygon id and monitor id

save(
  buffer_intersections, 
  file = 
    file.path(
      "intermediate_data", 
      "buffer_intersections"
    )
)

# remove variables
rm(
  projected_tracts, 
  buffers,
  one_tract, 
  one_buffer_size, 
  buffers_in_tract, 
  concordance_vector, 
  buffer_intersections
)
invisible(gc())

```


```{r make buffer intersection function from the above single case}

buffintersector <- 
  function(tract_geoid_as_character, 
           buffer_sf, 
           tract_sf) {
    
    # extract one buffer layer and simplify it
    one_buffer_size <- 
      buffer_sf %>% 
      select(monitor_id)
    
    # extract one tract and simplify it
    one_tract <- 
      tract_sf %>% 
      filter(geoid10 == tract_geoid_as_character) %>% 
      select(geoid10)
    
    # clip buffers to only the one tract
    buffers_in_tract <- 
      one_buffer_size %>% 
      st_intersection(one_tract, .) %>%  # this clips the buffers (buffer sf is .)
      rowid_to_column("polygon_id")  # add row numbers to id polygons in next step
    
    # check if nrow from the above is zero, and if so, make a tibble and return it
    if(nrow(buffers_in_tract) == 0) {
      null_tibble <- 
        tibble(geoid10 = tract_geoid_as_character, 
               n.overlaps = as.integer(0), 
               origins = as.integer(0), 
               area = as.numeric(0), 
               monitor_id_overlaps = "none", 
               geometry = "none")

      return(null_tibble)
      
    } else {
    
    # make concordance vector to replace "origin" values (rows) with monitor ids
    concordance_vector <- 
      setNames(
        object = buffers_in_tract$monitor_id,  # object to named
        nm = buffers_in_tract$polygon_id  # names (polygon #s)
      )
    
    # find buffer intersections and areas by monitor
    buffer_intersections <- 
      buffers_in_tract %>% 
      st_intersection() %>% 
      mutate(area = st_area(geometry),  # add areas
             monitor_id_overlaps =  # add column of monitor ids
               map(
                 .x = origins,  # go through the origin column (with polygon #s)
                 .f = function(a) concordance_vector[a]  # replace with monitor ids
                 )
             ) %>% 
      select(-c(polygon_id, monitor_id))  # remove origin polygon id and monitor id
    
    return(buffer_intersections)
    
    }
  }
 
# save the function to disk
save(
  buffintersector, 
  file = 
    file.path(
      "functions", 
      "buffintersector.rdata"
    )
)

rm(buffintersector)
invisible(gc())

```


```{r apply buffer function to a single case}

# load buffintersector function
load(
  file = 
    file.path(
      "functions", 
      "buffintersector.rdata"
    )
)

# load projected tracts data
load(
  file = 
    file.path(
      "intermediate_data", 
      "projected_tracts.rdata"
    )
)

# load buffers
load(
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# try the function on a single tract and buffer size
try_the_first <- 
  buffintersector(tract_geoid_as_character = "02013000100", 
                  buffer_sf = buffers$buffer_1000_m, 
                  tract_sf = projected_tracts)
```

```{r intra-tract buffer intersections all tracts one distance}

# load buffintersector function
load(
  file = 
    file.path(
      "functions", 
      "buffintersector.rdata"
    )
)

# load projected tracts data
load(
  file = 
    file.path(
      "intermediate_data", 
      "projected_tracts.rdata"
    )
)

# load buffers
load(
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# get list of tract geoids
tract_geoid_characters <- 
  projected_tracts %>% 
  pull(geoid10)

# # get buffer file of interest
# buffer_1000_m <- 
#   buffers$buffer_1000_m

# # apply buffer function for all tracts for 1000-meter buffer (TIME SUCK)
# buffer_intersections_1000_m <- 
#   map(
#     .f = buffintersector, 
#     .x = tract_geoid_characters,
#     buffer_sf = buffers$buffer_1000_m,
#     tract_sf = projected_tracts
#   )
# 
# # save the result
# save(
#   buffer_intersections_1000_m, 
#   file = 
#     file.path(
#       "intermediate_data",
#       "buffer_intersections_1000_m.rdata"
#       )
#   )
# 
# # remove object from memory
# rm(buffer_intersections_1000_m)
# gc()

```

```{r make ueber-function to do buffer intersections, eval=FALSE, include=FALSE}








# apply buffer function for all tracts for 2000-meter buffer
buffer_intersections_2000_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_2000_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()

# apply buffer function for all tracts for 2500-meter buffer
buffer_intersections_2500_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_2500_m,
    tract_sf = projected_tracts
  )

# apply buffer function for all tracts for 3000-meter buffer
buffer_intersections_3000_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_3000_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()

# apply buffer function for all tracts for 4000-meter buffer
buffer_intersections_4000_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_4000_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()



# apply buffer function for all tracts for 5000-meter buffer
buffer_intersections_5000_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_5000_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()



# apply buffer function for all tracts for 7500-meter buffer
buffer_intersections_7500_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_7500_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()



# apply buffer function for all tracts for 10000-meter buffer
buffer_intersections_10000_m <- 
  map(
    .f = buffintersector, 
    .x = tract_geoid_characters,
    buffer_sf = buffers$buffer_10000_m,
    tract_sf = projected_tracts
  )

# save the result
save(
  buffer_intersections_1000_m, 
  file = 
    file.path(
      "intermediate_data",
      "buffer_intersections_1000_m.rdata"
      )
  )

# remove object from memory
rm(buffer_intersections_1000_m)
gc()


  
```


```{r make function to do all at once}

# make map_buff_intersect function!
mapbuffintersector <- 
  function(distance_character_vector) {
    
    # requires loaded objects 
    #  "buffintersector"
    #  "buffers"
    #  "projected_tracts"
    #  "tract_geoid_characters"
    
    object_name_string <- 
      str_c("buffer_intersections_", distance_character_vector, "_m")
    buffer_subfile_name_string <- 
      str_c("buffer_", distance_character_vector, "_m")
    save_file_name_string <- 
      str_c("buffer_intersections_", distance_character_vector, "_m.rdata")
    
    buffer_subfile_sf <- 
      buffers[[buffer_subfile_name_string]]
      
    # apply buffer intersection function
    intersections <- 
      map(
        .f = buffintersector, 
        .x = tract_geoid_characters,
        buffer_sf = buffer_subfile_sf,
        tract_sf = projected_tracts
      )

    assign(object_name_string, intersections)

    # save the result
    save(
      object_name_string, 
      file = 
        file.path(
          "intermediate_data",
          save_file_name_string
          )
      )
  }

save(
  mapbuffintersector, 
  file = 
    file.path(
      "functions", 
      "mapbuffintersector.rdata"
    )
)

rm(mapbuffintersector)
invisible(gc())
 
```


```{r try the function}

# load buffintersector function
load(
  file = 
    file.path(
      "functions", 
      "buffintersector.rdata"
    )
)

# load mapbuffintersector function
load(
  file = 
    file.path(
      "functions", 
      "mapbuffintersector.rdata"
    )
)

# load projected tracts data
load(
  file = 
    file.path(
      "intermediate_data", 
      "projected_tracts.rdata"
    )
)

# load buffers
load(
  file = 
    file.path(
      "intermediate_data", 
      "buffers.rdata"
    )
)

# list of buffer distances desired in meters
distances <- 
  c("1000", 
    "2000", 
    "2500", 
    "3000", 
    "4000", 
    "5000", 
    "7500", 
    "10000")

# get list of tract geoids
tract_geoid_characters <- 
  projected_tracts %>% 
  pull(geoid10)

# mapbuffintersector("1000")

# apply the mapbuffintersector en masse
map(
  .x = distances, 
  .f = mapbuffintersector
)


```
